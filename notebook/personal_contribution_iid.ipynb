{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sE6fVS9IciUj"
      },
      "source": [
        "# Personal Contribution IID"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bARjH2gv4Ws3"
      },
      "source": [
        "## Introduction :\n",
        "This notebook contains experiments for the personal contribution on alternative gradient masking strategies applied for iid model editing (train_most_important, magnitude_most, magnitude_least and random)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UxkC1TZu4dml"
      },
      "source": [
        "## Import + setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--r-lLjm4aJc"
      },
      "outputs": [],
      "source": [
        "!pip install wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWm9hzq44lqG"
      },
      "outputs": [],
      "source": [
        "import wandb\n",
        "wandb.login()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A2sKuKgJ4wNP"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import torchvision\n",
        "import wandb\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple,  Optional, Iterable\n",
        "from torch.utils.data import random_split, Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from torch.optim import SGD\n",
        "import types\n",
        "import os\n",
        "import json\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOS-RXE04n5H"
      },
      "outputs": [],
      "source": [
        "print(\"GPU available :\", torch.cuda.is_available())\n",
        "print(\"Nom GPU\", torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"not available\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh7LDykl44QI"
      },
      "source": [
        "## Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zCHva71Q43es"
      },
      "outputs": [],
      "source": [
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3EXTlegD46w_"
      },
      "outputs": [],
      "source": [
        "def iid_shard_train_val(dataset, K, val_split=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Stratified IID sharding: each client receives (approximately) the same number of examples for each class.\n",
        "    Args:\n",
        "      dataset: PyTorch Dataset (WITHOUT applied transform)\n",
        "      K: number of clients\n",
        "      val_split: local validation fraction (within each client)\n",
        "      seed: for reproducibility\n",
        "    Returns:\n",
        "      client_data: {client_id: {'train': [idxs], 'val': [idxs]}}\n",
        "    \"\"\"\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
        "    n_classes = len(np.unique(labels))\n",
        "    class_indices = {c: np.where(labels == c)[0] for c in range(n_classes)}\n",
        "    for c in class_indices:\n",
        "        rng.shuffle(class_indices[c])\n",
        "\n",
        "    # Number of examples per class to be distributed to each client\n",
        "    examples_per_class = {c: len(class_indices[c]) // K for c in class_indices}\n",
        "    # Any \"leftovers\" (if not evenly divisible) are assigned first\n",
        "    leftovers = {c: len(class_indices[c]) % K for c in class_indices}\n",
        "\n",
        "    client_indices = {i: [] for i in range(K)}\n",
        "    for c in range(n_classes):\n",
        "        idxs = class_indices[c]\n",
        "        cursor = 0\n",
        "        for i in range(K):\n",
        "            take = examples_per_class[c] + (1 if i < leftovers[c] else 0)\n",
        "            client_indices[i].extend(idxs[cursor:cursor+take])\n",
        "            cursor += take\n",
        "\n",
        "    client_data = {}\n",
        "    for i in range(K):\n",
        "        idxs = np.array(client_indices[i])\n",
        "        rng.shuffle(idxs)\n",
        "        n_val = int(len(idxs) * val_split)\n",
        "        val_idxs = idxs[:n_val]\n",
        "        train_idxs = idxs[n_val:]\n",
        "        client_data[i] = {'train': train_idxs.tolist(), 'val': val_idxs.tolist()}\n",
        "    return client_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VPlCPNA448-t"
      },
      "outputs": [],
      "source": [
        "def non_iid_shard_train_val(dataset, K, Nc, val_split=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    Non-IID sharding (label-skew) + local train/val split for FL.\n",
        "    Each client receives Nc distinct classes (with no overlap),\n",
        "    then locally splits its data into train/val according to val_split.\n",
        "    Args:\n",
        "        dataset: PyTorch Dataset (without applied transform)\n",
        "        K: number of clients\n",
        "        Nc: number of different classes per client\n",
        "        val_split: local fraction for validation\n",
        "        seed: seed for reproducibility\n",
        "    Returns:\n",
        "      client_data: {client_id: {'train': [idxs], 'val': [idxs]}}\n",
        "    \"\"\"\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
        "    n_classes = len(np.unique(labels))\n",
        "    class_indices = {c: rng.permutation(np.where(labels == c)[0]).tolist() for c in range(n_classes)}\n",
        "    # Generate shards by class\n",
        "    shards_per_class = (K * Nc) // n_classes\n",
        "    shards = []\n",
        "    for c in range(n_classes):\n",
        "        idxs = class_indices[c]\n",
        "        shard_size = len(idxs) // shards_per_class\n",
        "        for i in range(shards_per_class):\n",
        "            shard = idxs[i*shard_size:(i+1)*shard_size]\n",
        "            if len(shard) > 0:\n",
        "                shards.append((c, shard))\n",
        "    rng.shuffle(shards)\n",
        "    # Assign Nc shards, each from a different class, to each client\n",
        "    client_shards = {i: [] for i in range(K)}\n",
        "    used = set()\n",
        "    for i in range(K):\n",
        "        chosen = []\n",
        "        class_seen = set()\n",
        "        for s_idx, (c, shard) in enumerate(shards):\n",
        "            if c not in class_seen and s_idx not in used:\n",
        "                chosen.append(s_idx)\n",
        "                class_seen.add(c)\n",
        "            if len(chosen) == Nc:\n",
        "                break\n",
        "        for s_idx in chosen:\n",
        "            used.add(s_idx)\n",
        "            client_shards[i].extend(shards[s_idx][1])\n",
        "    # Local split train/val for each client\n",
        "    client_data = {}\n",
        "    for i in range(K):\n",
        "        idxs = np.array(client_shards[i])\n",
        "        rng.shuffle(idxs)\n",
        "        n_val = int(len(idxs) * val_split)\n",
        "        val_idxs = idxs[:n_val]\n",
        "        train_idxs = idxs[n_val:]\n",
        "        client_data[i] = {'train': train_idxs.tolist(), 'val': val_idxs.tolist()}\n",
        "    return client_data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJ9hAy3e5HeU"
      },
      "outputs": [],
      "source": [
        "class DinoViT_CIFAR100(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model('vit_small_patch16_224.dino', pretrained=True)\n",
        "        # the dimension of features for ViT-S/16 is always 384 (see doc timm/models/vit.py)\n",
        "        self.backbone.head = nn.Identity()  # takes off head DINO\n",
        "        self.classifier = nn.Linear(384, num_classes)\n",
        "    def forward(self, x):\n",
        "        # timm ViT returns (batch, 384) if the head is nn.Identity\n",
        "        feats = self.backbone(x)   # (batch, 384)\n",
        "        out = self.classifier(feats)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2PE03Vzm5xxe"
      },
      "outputs": [],
      "source": [
        "def _num_total_params(mask: Dict[str, torch.Tensor]) -> int:\n",
        "    \"\"\"Returns the total number of parameters (elements) across all tensors in the mask.\"\"\"\n",
        "    return sum(t.numel() for t in mask.values())\n",
        "\n",
        "def _num_zero_params(mask: Dict[str, torch.Tensor]) -> int:\n",
        "    \"\"\"Returns the number of parameters set to zero in the mask (i.e., masked out).\"\"\"\n",
        "    return sum((t == 0).sum().item() for t in mask.values())\n",
        "\n",
        "def _compute_sparsity(mask: Dict[str, torch.Tensor]) -> float:\n",
        "    \"\"\"Returns the sparsity, i.e., the fraction of parameters that are masked (value in [0, 1]).\"\"\"\n",
        "    return _num_zero_params(mask) / _num_total_params(mask)\n",
        "\n",
        "\n",
        "def _compute_approximated_fisher_scores(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    device: torch.device,\n",
        "    num_batches: Optional[int] = None,\n",
        "    mask: Optional[Dict[str, torch.Tensor]] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Approximate the diagonal of the Fisher Information Matrix via empirical average.\n",
        "    Args:\n",
        "        model: torch.nn.Module\n",
        "        dataloader: DataLoader (local client data)\n",
        "        loss_fn: torch.nn loss function (e.g. nn.CrossEntropyLoss())\n",
        "        device: torch.device\n",
        "        num_batches: number of batches to use for approximation\n",
        "    Returns:\n",
        "        Dict {param_name: tensor of Fisher diagonal}\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    fisher_diag = {\n",
        "        name: torch.zeros_like(param, device=device)\n",
        "        for name, param in model.named_parameters()\n",
        "        if param.requires_grad\n",
        "    }\n",
        "    total_batches = len(dataloader) if num_batches is None else num_batches\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(\n",
        "        tqdm(dataloader, total=total_batches, desc=\"Computing Fisher\")\n",
        "    ):\n",
        "        if num_batches is not None and batch_idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                fisher_diag[name] += param.grad.detach() ** 2\n",
        "                if mask is not None:\n",
        "                    fisher_diag[name] *= mask[name]\n",
        "\n",
        "    for name in fisher_diag:\n",
        "        fisher_diag[name] /= total_batches\n",
        "\n",
        "    return fisher_diag\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxDoHJ7s6ekN"
      },
      "source": [
        "## Adjusted strategies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m3cKf8xN6dwV"
      },
      "outputs": [],
      "source": [
        "def calibrate_gradient_mask_progressive(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    sparsity: float = 0.9,\n",
        "    rounds: int = 5,\n",
        "    num_batches: Optional[int] = None,\n",
        "    loss_fn: nn.Module = nn.CrossEntropyLoss(),\n",
        "    approximate_fisher: bool = True,\n",
        "    strategy: str = \"train_least_important\",\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Progressive mask calibration via Fisher scores, magnitude, or random strategy.\n",
        "    Returns a dict {param_name: binary mask tensor}.\n",
        "    \"\"\"\n",
        "    print(\"*\" * 50)\n",
        "    print(f\"Progressive Mask Calibration - Strategy: {strategy}\")\n",
        "    print(\"*\" * 50)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    mask = {\n",
        "        n: torch.ones_like(p, device=device)\n",
        "        for n, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    }\n",
        "\n",
        "    for r in range(1, rounds + 1):\n",
        "        print(f\"[Round {r}]\")\n",
        "\n",
        "        # Score computation (only approximate Fisher supported here)\n",
        "        if approximate_fisher:\n",
        "            scores = _compute_approximated_fisher_scores(\n",
        "                model=model,\n",
        "                dataloader=dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                num_batches=num_batches,\n",
        "                device=device,\n",
        "                mask=mask,\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(\"Only approximate Fisher is implemented.\")\n",
        "\n",
        "         # 1. take every scores (to log, debug)\n",
        "        all_scores = torch.cat([v.flatten() for v in scores.values()])\n",
        "        # 2. Retain only the scores of parameters that remain active (i.e., where mask == 1)\n",
        "        active_scores = torch.cat([\n",
        "            score[mask[name] != 0].flatten()\n",
        "            for name, score in scores.items()\n",
        "        ])\n",
        "        total_params = all_scores.numel()\n",
        "        total_active_params = active_scores.numel()\n",
        "\n",
        "        # Exponentially decrease keep_fraction for progressive pruning\n",
        "        keep_fraction = (1-sparsity) ** (r / rounds)\n",
        "        k = int(keep_fraction * total_params)\n",
        "        print(f\"Current keep fraction: {keep_fraction:.4f} | Keeping only top k: {k}\")\n",
        "\n",
        "        if strategy == \"train_least_important\":\n",
        "            #To prevent bugs: ensure that k does not exceed the number of active parameters\n",
        "            k = max(1, min(k, total_active_params))\n",
        "            threshold, _ = torch.kthvalue(active_scores, k)\n",
        "            print(\"Threshold (below which params are kept):\", threshold)\n",
        "            for name, score in scores.items():\n",
        "                # Mask only newly selected parameters; keep previously zeroed (masked) ones unchanged\n",
        "                new_mask = (score <= threshold).float()\n",
        "                mask[name] = mask[name] * new_mask\n",
        "\n",
        "        elif strategy == \"train_most_important\":\n",
        "            # To prevent errors: k must not exceed the number of currently active parameters\n",
        "            k = max(1, min(k, total_active_params))\n",
        "            threshold, _ = torch.kthvalue(active_scores, k)\n",
        "            print(\"Threshold (below which params are kept):\", threshold)\n",
        "            for name, score in scores.items():\n",
        "                # Mask only new parameters; retain previously masked zeros\n",
        "                new_mask = (score >= threshold).float()\n",
        "                mask[name] = mask[name] * new_mask\n",
        "\n",
        "        elif strategy == \"random\":\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.requires_grad:\n",
        "                    num_params = param.numel()\n",
        "                    k = int((1 - sparsity) * num_params)\n",
        "                    random_scores = torch.rand_like(param)\n",
        "                    threshold, _ = torch.kthvalue(random_scores.flatten(), k)\n",
        "                    mask[name] = (random_scores > threshold).float()\n",
        "\n",
        "        elif strategy == \"magnitude_most\":\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.requires_grad:\n",
        "                    num_params = param.numel()\n",
        "                    k = int((1 - sparsity) * num_params)\n",
        "                    abs_weights = param.detach().abs()\n",
        "                    threshold, _ = torch.kthvalue(abs_weights.flatten(), num_params - k + 1)\n",
        "                    mask[name] = (abs_weights >= threshold).float()\n",
        "\n",
        "        elif strategy == \"magnitude_least\":\n",
        "            for name, param in model.named_parameters():\n",
        "                if param.requires_grad:\n",
        "                    num_params = param.numel()\n",
        "                    k = int((1 - sparsity) * num_params)\n",
        "                    abs_weights = param.detach().abs()\n",
        "                    # Retain the k parameters with the smallest absolute values (lowest-magnitude weights)\n",
        "                    threshold, _ = torch.kthvalue(abs_weights.flatten(), k)\n",
        "                    mask[name] = (abs_weights <= threshold).float()\n",
        "\n",
        "\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "\n",
        "        print(\n",
        "            f\"After round {r} mask sparsity: { _compute_sparsity(mask):.4f} \"\n",
        "            f\"({_num_zero_params(mask)}/{_num_total_params(mask)} zeroed params)\"\n",
        "        )\n",
        "        print()\n",
        "\n",
        "    print(\"Progressive Mask Calibration completed.\")\n",
        "    return mask\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRhzd5u27IFb"
      },
      "source": [
        "## Sparsity"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xd8gAeK6wc5"
      },
      "outputs": [],
      "source": [
        "class SparseSGDM(SGD):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[torch.nn.Parameter],\n",
        "        named_params: Dict[str, torch.nn.Parameter],\n",
        "        lr: float,\n",
        "        momentum: float = 0.0,\n",
        "        weight_decay: float = 0.0,\n",
        "        mask: Dict[str, torch.Tensor] = None,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            params,\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            weight_decay=weight_decay,\n",
        "        )\n",
        "        self.mask = mask  # Dict {param_name: mask_tensor}\n",
        "        self.named_params = named_params  # Dict {name: param}\n",
        "        self.param_id_to_name = {id(p): n for n, p in named_params.items()}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                name = self.param_id_to_name.get(id(p))\n",
        "                if p.grad is not None and self.mask is not None and name in self.mask:\n",
        "                    p.grad.mul_(self.mask[name])\n",
        "\n",
        "        return super().step(closure)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRqj4o0a7PYK"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t371AoIU7PyT"
      },
      "outputs": [],
      "source": [
        "def sparse_fine_tune(\n",
        "    model: nn.Module,\n",
        "    dataloader,\n",
        "    device,\n",
        "    mask,\n",
        "    lr=1e-3,\n",
        "    epochs=1,\n",
        "    momentum=0.9,\n",
        "    weight_decay=5e-4,\n",
        "):\n",
        "    \"\"\"\n",
        "    Sparse fine-tuning of a model using a fixed binary mask.\n",
        "    Only parameters where mask == 1 are updated (others are frozen).\n",
        "    \"\"\"\n",
        "    model.to(device)\n",
        "    # Set requires_grad according to the mask\n",
        "    for name, param in model.named_parameters():\n",
        "        if name in mask:\n",
        "            param.requires_grad = (mask[name] == 1).any().item()\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Prepare params to optimize (only those with requires_grad)\n",
        "    named_params = dict(model.named_parameters())\n",
        "    params = [p for p  in named_params.values() if p.requires_grad]\n",
        "\n",
        "    # SGD standard (no need for custom optimizer since masking is done by requires_grad)\n",
        "    optimizer = SparseSGDM(\n",
        "        params=params,\n",
        "        named_params=named_params,\n",
        "        lr=lr,\n",
        "        momentum=momentum,\n",
        "        weight_decay=weight_decay,\n",
        "        mask=mask,\n",
        "    )\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Optional: reset requires_grad to True for all params if you reuse model elsewhere\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObFldsf57lrN"
      },
      "source": [
        "## Adjusted Client Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMPzvp-g7nuR"
      },
      "outputs": [],
      "source": [
        "class Client:\n",
        "    \"\"\"\n",
        "    Client class representing a federated learning participant.\n",
        "\n",
        "    Each client holds a private dataset and performs local training or sparse fine-tuning\n",
        "    on a copy of the global model. It supports standard local updates as well as\n",
        "    gradient-masked sparse updates using a calibrated mask.\n",
        "    \"\"\"\n",
        "    def __init__(self, client_id, dataset, device):\n",
        "        self.client_id = client_id\n",
        "        self.dataset = dataset\n",
        "        self.device = device\n",
        "        self.last_mask = None  # Store the mask if needed\n",
        "\n",
        "    def calibrate_mask(\n",
        "        self,\n",
        "        model,\n",
        "        sparsity_ratio=0.9,\n",
        "        num_calib_rounds=5,\n",
        "        batch_size=128,\n",
        "        num_batches: Optional[int] = None,\n",
        "        loss_fn=None,\n",
        "        strategy: str = \"train_least_important\",\n",
        "    ):\n",
        "        \"\"\"Calibrate a binary mask based on importance strategy (Fisher, magnitude, or random).\"\"\"\n",
        "        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "        if loss_fn is None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "        mask = calibrate_gradient_mask_progressive(\n",
        "            model=model,\n",
        "            dataloader=dataloader,\n",
        "            device=self.device,\n",
        "            sparsity=sparsity_ratio,\n",
        "            rounds=num_calib_rounds,\n",
        "            num_batches=num_batches,\n",
        "            loss_fn=loss_fn,\n",
        "            approximate_fisher=True,\n",
        "            strategy=strategy,\n",
        "        )\n",
        "        self.last_mask = mask\n",
        "        return mask\n",
        "\n",
        "    def apply_mask_requires_grad(self, model, mask):\n",
        "        \"\"\"\n",
        "        Sets requires_grad=True for params where mask == 1, False otherwise.\n",
        "        \"\"\"\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in mask:\n",
        "                param.requires_grad = (mask[name] == 1).any().item()\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def sparse_fine_tune(\n",
        "        self,\n",
        "        model,\n",
        "        mask,\n",
        "        lr=1e-3,\n",
        "        epochs=1,\n",
        "        batch_size=128,\n",
        "        momentum=0.9,\n",
        "        weight_decay=5e-4,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Sparse fine-tuning: only params with requires_grad=True (i.e. mask==1) are updated.\n",
        "        \"\"\"\n",
        "        self.apply_mask_requires_grad(model, mask)\n",
        "        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        optimizer = torch.optim.SGD(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            weight_decay=weight_decay,\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            for inputs, targets in dataloader:\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Reset requires_grad\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def local_train(\n",
        "        self,\n",
        "        global_model,\n",
        "        epochs,\n",
        "        batch_size,\n",
        "        lr,\n",
        "        momentum,\n",
        "        weight_decay,\n",
        "        scheduler_fn=None,\n",
        "        use_sparse=False,\n",
        "        sparsity_ratio=0.9,\n",
        "        num_calib_rounds=5,\n",
        "        num_batches: Optional[int] = None,\n",
        "        sparse_ft_epochs=1,\n",
        "        strategy: str = \"train_least_important\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Performs standard local training or (if use_sparse) sparse fine-tuning.\n",
        "        \"\"\"\n",
        "        model = DinoViT_CIFAR100(num_classes=100).to(self.device)\n",
        "        model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "        if use_sparse:\n",
        "            mask = self.calibrate_mask(\n",
        "                model,\n",
        "                sparsity_ratio=sparsity_ratio,\n",
        "                num_calib_rounds=num_calib_rounds,\n",
        "                batch_size=batch_size,\n",
        "                num_batches=num_batches,\n",
        "                strategy=strategy,  # ← AJOUT\n",
        "            )\n",
        "            self.sparse_fine_tune(\n",
        "                model,\n",
        "                mask,\n",
        "                lr=lr,\n",
        "                epochs=sparse_ft_epochs,\n",
        "                batch_size=batch_size,\n",
        "                momentum=momentum,\n",
        "                weight_decay=weight_decay,\n",
        "            )\n",
        "        else:\n",
        "            # Standard local training\n",
        "            loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "            optimizer = torch.optim.SGD(\n",
        "                filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                lr=lr,\n",
        "                momentum=momentum,\n",
        "                weight_decay=weight_decay,\n",
        "            )\n",
        "            scheduler = scheduler_fn(optimizer) if scheduler_fn else None\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            model.train()\n",
        "            for epoch in range(epochs):\n",
        "                for X, y in loader:\n",
        "                    X, y = X.to(self.device), y.to(self.device)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = criterion(model(X), y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                if scheduler:\n",
        "                    scheduler.step()\n",
        "\n",
        "        return model.state_dict()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3NYEI3m8kuj"
      },
      "source": [
        "## Adjusted Federated Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZtC5aFa8obO"
      },
      "outputs": [],
      "source": [
        "class FederatedTrainer:\n",
        "    \"\"\"\n",
        "    Orchestrates federated learning (FedAvg) with optional sparse model editing.\n",
        "    Supports both IID and non-IID client splits (weighted aggregation).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        clients,\n",
        "        global_model,\n",
        "        device,\n",
        "        client_fraction,\n",
        "        local_epochs,\n",
        "        batch_size,\n",
        "        lr,\n",
        "        momentum,\n",
        "        weight_decay,\n",
        "        scheduler_fn=None,\n",
        "        use_sparse=False,\n",
        "        sparsity_ratio=0.9,\n",
        "        num_calib_rounds=5,\n",
        "        num_batches: Optional[int] = None,\n",
        "        sparse_ft_epochs=1,\n",
        "        strategy: str = \"train_least_important\",\n",
        "    ):\n",
        "        self.clients = clients\n",
        "        self.global_model = global_model\n",
        "        self.device = device\n",
        "        self.client_fraction = client_fraction\n",
        "        self.local_epochs = local_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "        self.scheduler_fn = scheduler_fn\n",
        "\n",
        "        self.use_sparse = use_sparse\n",
        "        self.sparsity_ratio = sparsity_ratio\n",
        "        self.num_calib_rounds = num_calib_rounds\n",
        "        self.num_batches = num_batches\n",
        "        self.sparse_ft_epochs = sparse_ft_epochs\n",
        "        self.strategy = strategy\n",
        "\n",
        "    def aggregate_weights(self, client_states, client_sizes):\n",
        "        \"\"\"\n",
        "        Weighted average (FedAvg) of the selected client weights.\n",
        "        client_states: list of state_dicts (one per client)\n",
        "        client_sizes: list of int (number of samples per client)\n",
        "        \"\"\"\n",
        "        total = sum(client_sizes)\n",
        "        avg_state = {}\n",
        "        for key in client_states[0].keys():\n",
        "            weighted_sum = sum(state[key].float() * size for state, size in zip(client_states, client_sizes))\n",
        "            avg_state[key] = weighted_sum / total\n",
        "        return avg_state\n",
        "\n",
        "    def train_round(self):\n",
        "        \"\"\"\n",
        "        Runs one FedAvg round with optional model editing (sparse fine-tune).\n",
        "        Aggregates using sample-weighted mean (FedAvg-style).\n",
        "        \"\"\"\n",
        "        num_clients = len(self.clients)\n",
        "        m = max(int(self.client_fraction * num_clients), 1)\n",
        "        selected = np.random.choice(self.clients, m, replace=False)\n",
        "        client_states = []\n",
        "        client_sizes = []\n",
        "\n",
        "        for client in selected:\n",
        "            client_state = client.local_train(\n",
        "                global_model=self.global_model,\n",
        "                epochs=self.local_epochs,\n",
        "                batch_size=self.batch_size,\n",
        "                lr=self.lr,\n",
        "                momentum=self.momentum,\n",
        "                weight_decay=self.weight_decay,\n",
        "                scheduler_fn=self.scheduler_fn,\n",
        "                use_sparse=self.use_sparse,\n",
        "                sparsity_ratio=self.sparsity_ratio,\n",
        "                num_calib_rounds=self.num_calib_rounds,\n",
        "                num_batches=self.num_batches,\n",
        "                sparse_ft_epochs=self.sparse_ft_epochs,\n",
        "                strategy=self.strategy  # ← AJOUT\n",
        "            )\n",
        "            client_states.append(client_state)\n",
        "            client_sizes.append(len(client.dataset))\n",
        "\n",
        "        avg_state = self.aggregate_weights(client_states, client_sizes)\n",
        "        self.global_model.load_state_dict(avg_state)\n",
        "\n",
        "    def fit(self, n_rounds, eval_fn=None, eval_every=1):\n",
        "        for rnd in range(1, n_rounds + 1):\n",
        "            print(f'---- FedAvg Round {rnd} {\"(SPARSE-EDITING)\" if self.use_sparse else \"\"} ----')\n",
        "            self.train_round()\n",
        "            if eval_fn and (rnd % eval_every == 0 or rnd == n_rounds):\n",
        "                acc, loss = eval_fn(self.global_model)\n",
        "                print(f'[Round {rnd}] Eval: Acc={acc:.3f} | Loss={loss:.3f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IcInNSTm81gT"
      },
      "source": [
        "## Other Utils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LEKUwvDE83-Q"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    Evaluates a classification model on a given dataset.\n",
        "    Returns (accuracy, average_loss).\n",
        "    \"\"\"\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    correct, total, total_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            total_loss += loss.item() * X.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += X.size(0)\n",
        "    if total == 0:\n",
        "        return 0.0, 0.0\n",
        "    return correct / total, total_loss / total\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0T1BIb2Aj4nT"
      },
      "source": [
        "## Hyperparameters Research"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ArwG-kM6BGWg"
      },
      "outputs": [],
      "source": [
        "# Hyperparams\n",
        "K = 100\n",
        "val_split = 0.2\n",
        "seed = 42\n",
        "Nc = 50\n",
        "\n",
        "# --- Dataset (no transform here)\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- Freeze the IID sharding (client split and local train/val split)\n",
        "iid_split = iid_shard_train_val(full_train, K=K, val_split=val_split, seed=seed)\n",
        "\n",
        "#--- Freeze the NON-IID sharding (client split and local train/val split)\n",
        "non_iid_split = non_iid_shard_train_val(full_train, K=K, Nc=Nc, val_split=val_split, seed=seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9BYSPIRYffM"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparams\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 10\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "\n",
        "# Param grid\n",
        "sparsity_ratios = [0.85, 0.90, 0.95]\n",
        "num_calib_rounds_list = [1]\n",
        "sparse_ft_epochs = 1\n",
        "strategy = \"magnitude_least\"  # or \"random\", \"magnitude_most\", \"magnitude_least\"\n",
        "\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Data (no split/transform here) ---\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "iid_split = iid_shard_train_val(full_train, K=K, val_split=0.2, seed=42)\n",
        "\n",
        "# --- Wrapper for transforms after split ---\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "# --- Validation globale ---\n",
        "val_indices = np.concatenate([iid_split[i]['val'] for i in range(K)])\n",
        "val_set = TransformedSubset(Subset(full_train, val_indices), val_transform)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def eval_fn_edit(model):\n",
        "    return evaluate(model, val_loader, device)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']}) from {path}\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def save_best_hyperparams(acc_history, config, path):\n",
        "    best_acc = max(acc_history) if acc_history else 0.0\n",
        "    run_data = {\"best_val_acc\": best_acc}\n",
        "    run_data.update(config)\n",
        "    with open(path, \"a\") as f:\n",
        "        f.write(json.dumps(run_data) + \"\\n\")\n",
        "\n",
        "def fit_with_wandb_and_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=None, best_json_path=None, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    if resume and checkpoint_path:\n",
        "        start_round, acc_history, loss_history = load_checkpoint(self.global_model, checkpoint_path)\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} (SPARSE-EDITING) ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            acc, loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Eval: Acc={acc:.3f} | Loss={loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"val_acc\": acc, \"val_loss\": loss})\n",
        "            acc_history.append(acc)\n",
        "            loss_history.append(loss)\n",
        "            if checkpoint_path and (rnd % 5 == 0 or rnd == n_rounds):\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "    if best_json_path:\n",
        "        save_best_hyperparams(acc_history, wandb.config, best_json_path)\n",
        "\n",
        "# === GRIDSEARCH MODEL EDITING FL-READY ===\n",
        "run_idx = 0\n",
        "for sparsity_ratio in sparsity_ratios:\n",
        "    for num_calib_rounds in num_calib_rounds_list:\n",
        "        run_idx += 1\n",
        "        print(f\"\\n=== MODEL EDITING RUN {run_idx}/3 ===\\n\")\n",
        "        clients_edit = []\n",
        "        for i in range(K):\n",
        "            train_idxs = iid_split[i]['train']\n",
        "            client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "            clients_edit.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "        global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "        run_name = (f\"model_editing_iid_{strategy}_nrounds{n_rounds}_lr{lr}_\"\n",
        "                    f\"sp{int(sparsity_ratio*100)}_calib{num_calib_rounds}_ftep{sparse_ft_epochs}\")\n",
        "        checkpoint_path = f\"{run_name}.pt\"\n",
        "        best_json_path = f\"{run_name}.json\"\n",
        "\n",
        "        wandb.init(\n",
        "            project=\"fl-fedavg-personnal-contribution\",\n",
        "            name=run_name,\n",
        "            config={\n",
        "                \"model\": \"DINO ViT-S/16\",\n",
        "                \"K\": K,\n",
        "                \"C\": C,\n",
        "                \"J\": J,\n",
        "                \"n_rounds\": n_rounds,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"lr\": lr,\n",
        "                \"momentum\": momentum,\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"sharding\": \"iid\",\n",
        "                \"use_sparse\": True,\n",
        "                \"sparsity_ratio\": sparsity_ratio,\n",
        "                \"num_calib_rounds\": num_calib_rounds,\n",
        "                \"sparse_ft_epochs\": sparse_ft_epochs,\n",
        "                \"strategy\" : strategy\n",
        "            }\n",
        "        )\n",
        "\n",
        "        trainer_edit = FederatedTrainer(\n",
        "            clients=clients_edit,\n",
        "            global_model=global_model,\n",
        "            device=device,\n",
        "            client_fraction=C,\n",
        "            local_epochs=J,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            weight_decay=weight_decay,\n",
        "            scheduler_fn=make_scheduler,\n",
        "            use_sparse=True,\n",
        "            sparsity_ratio=sparsity_ratio,\n",
        "            num_calib_rounds=num_calib_rounds,\n",
        "            sparse_ft_epochs=sparse_ft_epochs,\n",
        "            strategy=strategy\n",
        "        )\n",
        "        trainer_edit.fit = types.MethodType(fit_with_wandb_and_logs, trainer_edit)\n",
        "        trainer_edit.fit(\n",
        "            n_rounds,\n",
        "            eval_fn=eval_fn_edit,\n",
        "            eval_every=2,\n",
        "            checkpoint_path=checkpoint_path,\n",
        "            best_json_path=best_json_path,\n",
        "            resume=True\n",
        "        )\n",
        "        wandb.finish()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VnlxxpPkJkf"
      },
      "source": [
        "## Test Accuracy runs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KXTircaiAZlU"
      },
      "outputs": [],
      "source": [
        "# Hyperparams\n",
        "K = 100\n",
        "val_split = 0\n",
        "seed = 42\n",
        "Nc = 50\n",
        "\n",
        "# --- Dataset (no transform here)\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- Freeze sharding IID (split clients and local train/val = 0 here)\n",
        "iid_split = iid_shard_train_val(full_train, K=K, val_split=val_split, seed=seed)\n",
        "\n",
        "# --- freeze sharding NON-IID (split clients and local train/val = 0 here)\n",
        "non_iid_split = non_iid_shard_train_val(full_train, K=K, Nc=Nc, val_split=val_split, seed=seed)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fEKaVzmWAHZa"
      },
      "outputs": [],
      "source": [
        "# ====== FL READY: DATA, CLIENTS, WANDB, MODEL EDITING IID TEST ACC ======\n",
        "\n",
        "# --- FL params ---\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 20\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "\n",
        "strategy = \"train_least_important\"\n",
        "# --- Model Editing HP choosen ---\n",
        "sparsity_ratio = 0.85\n",
        "num_calib_rounds = 3\n",
        "sparse_ft_epochs = 1\n",
        "\n",
        "CHECKPOINT_PATH = \"model_editing_iid_checkpoint.pt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Loads CIFAR-100 brut (no transform here) ---\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- FL split: IID sharding + local train ---\n",
        "client_data = iid_split\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "# ---Instantiate clients prepared for FL (local training only) ---\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    train_idxs = client_data[i]['train']\n",
        "    client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "    clients.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "# --- Official test set ---\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    TransformedSubset(test_set, val_transform), batch_size=128, shuffle=False\n",
        ")\n",
        "\n",
        "# --- Global model ViT-S/16 DINO CIFAR-100 ---\n",
        "global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def eval_fn_test(model):\n",
        "    return evaluate(model, test_loader, device)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path=CHECKPOINT_PATH):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path=CHECKPOINT_PATH):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']})\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def plot_history(acc_history, loss_history, eval_every):\n",
        "    rounds = np.arange(0, len(acc_history))*eval_every + eval_every\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(rounds, acc_history, label='Test Acc')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(rounds, loss_history, label='Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def fit_with_all_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=CHECKPOINT_PATH, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    if resume:\n",
        "        start_round, acc_history, loss_history = load_checkpoint(self.global_model, checkpoint_path)\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} (MODEL EDITING) ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            test_acc, test_loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Test Acc={test_acc:.3f} | Test Loss={test_loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"test_acc\": test_acc, \"test_loss\": test_loss})\n",
        "            acc_history.append(test_acc)\n",
        "            loss_history.append(test_loss)\n",
        "            if rnd % 5 == 0 or rnd == n_rounds:\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "        if rnd % 5 == 0 or rnd == n_rounds:\n",
        "            plot_history(acc_history, loss_history, eval_every)\n",
        "\n",
        "# --- WANDB init ---\n",
        "wandb.init(\n",
        "    project=\"fl-fedavg-personnal-contribution\",\n",
        "    name=f\"model_editing_iid_test_acc_{strategy}_J{J}_nrounds{n_rounds}_lr{lr}_sp{int(sparsity_ratio*100)}_calib{num_calib_rounds}\",\n",
        "    config={\n",
        "        \"model\": \"DINO ViT-S/16\",\n",
        "        \"K\": K,\n",
        "        \"C\": C,\n",
        "        \"J\": J,\n",
        "        \"n_rounds\": n_rounds,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr,\n",
        "        \"momentum\": momentum,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"sharding\": \"iid\",\n",
        "        \"Nc\": None,\n",
        "        \"use_sparse\": True,\n",
        "        \"sparsity_ratio\": sparsity_ratio,\n",
        "        \"num_calib_rounds\": num_calib_rounds,\n",
        "        \"sparse_ft_epochs\": sparse_ft_epochs,\n",
        "        \"strategy\" : strategy\n",
        "    }\n",
        ")\n",
        "\n",
        "# --- Federated trainer FL-ready ---\n",
        "trainer = FederatedTrainer(\n",
        "    clients=clients,\n",
        "    global_model=global_model,\n",
        "    device=device,\n",
        "    client_fraction=C,\n",
        "    local_epochs=J,\n",
        "    batch_size=batch_size,\n",
        "    lr=lr,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    scheduler_fn=make_scheduler,\n",
        "    use_sparse=True,\n",
        "    sparsity_ratio=sparsity_ratio,\n",
        "    num_calib_rounds=num_calib_rounds,\n",
        "    sparse_ft_epochs=sparse_ft_epochs,\n",
        "    strategy = strategy\n",
        ")\n",
        "\n",
        "# --- Patch et run ---\n",
        "trainer.fit = types.MethodType(fit_with_all_logs, trainer)\n",
        "trainer.fit(n_rounds, eval_fn=eval_fn_test, eval_every=2, checkpoint_path=CHECKPOINT_PATH, resume=True)\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BwzSzBASJ_F5"
      },
      "outputs": [],
      "source": [
        "# ====== FL READY: DATA, CLIENTS, WANDB, MODEL EDITING IID TEST ACC ======\n",
        "\n",
        "# --- FL params ---\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 20\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "\n",
        "strategy = \"train_most_important\"\n",
        "# --- Model Editing HP choosen ---\n",
        "sparsity_ratio = 0.90\n",
        "num_calib_rounds = 5\n",
        "sparse_ft_epochs = 1\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Chargement CIFAR-100 brut (pas de transform ici) ---\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- FL split: IID sharding + train local ---\n",
        "client_data = iid_split\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "# --- Instanciation des clients FL-ready (train local uniquement) ---\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    train_idxs = client_data[i]['train']\n",
        "    client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "    clients.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "# --- Test set officiel (wrap pour normalisation identique à val) ---\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    TransformedSubset(test_set, val_transform), batch_size=128, shuffle=False\n",
        ")\n",
        "\n",
        "# --- Global model ViT-S/16 DINO CIFAR-100 ---\n",
        "global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def eval_fn_test(model):\n",
        "    return evaluate(model, test_loader, device)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path=CHECKPOINT_PATH):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path=CHECKPOINT_PATH):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']})\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def plot_history(acc_history, loss_history, eval_every):\n",
        "    rounds = np.arange(0, len(acc_history))*eval_every + eval_every\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(rounds, acc_history, label='Test Acc')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(rounds, loss_history, label='Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def fit_with_all_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=CHECKPOINT_PATH, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} (MODEL EDITING) ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            test_acc, test_loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Test Acc={test_acc:.3f} | Test Loss={test_loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"test_acc\": test_acc, \"test_loss\": test_loss})\n",
        "            acc_history.append(test_acc)\n",
        "            loss_history.append(test_loss)\n",
        "            if rnd % 5 == 0 or rnd == n_rounds:\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "        if rnd % 5 == 0 or rnd == n_rounds:\n",
        "            plot_history(acc_history, loss_history, eval_every)\n",
        "\n",
        "# --- WANDB init ---\n",
        "wandb.init(\n",
        "    project=\"fl-fedavg-personnal-contribution\",\n",
        "    name=f\"model_editing_iid_test_acc_{strategy}_J{J}_nrounds{n_rounds}_lr{lr}_sp{int(sparsity_ratio*100)}_calib{num_calib_rounds}\",\n",
        "    config={\n",
        "        \"model\": \"DINO ViT-S/16\",\n",
        "        \"K\": K,\n",
        "        \"C\": C,\n",
        "        \"J\": J,\n",
        "        \"n_rounds\": n_rounds,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr,\n",
        "        \"momentum\": momentum,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"sharding\": \"iid\",\n",
        "        \"Nc\": None,\n",
        "        \"use_sparse\": True,\n",
        "        \"sparsity_ratio\": sparsity_ratio,\n",
        "        \"num_calib_rounds\": num_calib_rounds,\n",
        "        \"sparse_ft_epochs\": sparse_ft_epochs,\n",
        "        \"strategy\" : strategy\n",
        "    }\n",
        ")\n",
        "\n",
        "# --- Federated trainer FL-ready ---\n",
        "trainer = FederatedTrainer(\n",
        "    clients=clients,\n",
        "    global_model=global_model,\n",
        "    device=device,\n",
        "    client_fraction=C,\n",
        "    local_epochs=J,\n",
        "    batch_size=batch_size,\n",
        "    lr=lr,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    scheduler_fn=make_scheduler,\n",
        "    use_sparse=True,\n",
        "    sparsity_ratio=sparsity_ratio,\n",
        "    num_calib_rounds=num_calib_rounds,\n",
        "    sparse_ft_epochs=sparse_ft_epochs,\n",
        "    strategy = strategy\n",
        ")\n",
        "\n",
        "# --- Patch et run ---\n",
        "trainer.fit = types.MethodType(fit_with_all_logs, trainer)\n",
        "trainer.fit(n_rounds, eval_fn=eval_fn_test, eval_every=2, checkpoint_path=CHECKPOINT_PATH, resume=True)\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AgkgSklkS3UR"
      },
      "outputs": [],
      "source": [
        "# ====== FL READY: DATA, CLIENTS, WANDB, MODEL EDITING IID TEST ACC ======\n",
        "\n",
        "# --- FL params ---\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 20\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "\n",
        "strategy = \"random\"\n",
        "# --- Model Editing HP choosen ---\n",
        "sparsity_ratio = 0.90\n",
        "num_calib_rounds = 1\n",
        "sparse_ft_epochs = 1\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Loads CIFAR-100 brut (no transform here) ---\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- FL split: IID sharding + train local ---\n",
        "client_data = iid_split\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "# ---Instantiate clients prepared for FL (local training only) ---\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    train_idxs = client_data[i]['train']\n",
        "    client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "    clients.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "# --- Official test set ---\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    TransformedSubset(test_set, val_transform), batch_size=128, shuffle=False\n",
        ")\n",
        "\n",
        "# --- Global model ViT-S/16 DINO CIFAR-100 ---\n",
        "global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def eval_fn_test(model):\n",
        "    return evaluate(model, test_loader, device)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path=CHECKPOINT_PATH):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path=CHECKPOINT_PATH):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']})\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def plot_history(acc_history, loss_history, eval_every):\n",
        "    rounds = np.arange(0, len(acc_history))*eval_every + eval_every\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(rounds, acc_history, label='Test Acc')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(rounds, loss_history, label='Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def fit_with_all_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=CHECKPOINT_PATH, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} (MODEL EDITING) ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            test_acc, test_loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Test Acc={test_acc:.3f} | Test Loss={test_loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"test_acc\": test_acc, \"test_loss\": test_loss})\n",
        "            acc_history.append(test_acc)\n",
        "            loss_history.append(test_loss)\n",
        "            if rnd % 5 == 0 or rnd == n_rounds:\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "        if rnd % 5 == 0 or rnd == n_rounds:\n",
        "            plot_history(acc_history, loss_history, eval_every)\n",
        "\n",
        "# --- WANDB init ---\n",
        "wandb.init(\n",
        "    project=\"fl-fedavg-personnal-contribution\",\n",
        "    name=f\"model_editing_iid_test_acc_{strategy}_J{J}_nrounds{n_rounds}_lr{lr}_sp{int(sparsity_ratio*100)}_calib{num_calib_rounds}\",\n",
        "    config={\n",
        "        \"model\": \"DINO ViT-S/16\",\n",
        "        \"K\": K,\n",
        "        \"C\": C,\n",
        "        \"J\": J,\n",
        "        \"n_rounds\": n_rounds,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr,\n",
        "        \"momentum\": momentum,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"sharding\": \"iid\",\n",
        "        \"Nc\": None,\n",
        "        \"use_sparse\": True,\n",
        "        \"sparsity_ratio\": sparsity_ratio,\n",
        "        \"num_calib_rounds\": num_calib_rounds,\n",
        "        \"sparse_ft_epochs\": sparse_ft_epochs,\n",
        "        \"strategy\" : strategy\n",
        "    }\n",
        ")\n",
        "\n",
        "# --- Federated trainer FL-ready ---\n",
        "trainer = FederatedTrainer(\n",
        "    clients=clients,\n",
        "    global_model=global_model,\n",
        "    device=device,\n",
        "    client_fraction=C,\n",
        "    local_epochs=J,\n",
        "    batch_size=batch_size,\n",
        "    lr=lr,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    scheduler_fn=make_scheduler,\n",
        "    use_sparse=True,\n",
        "    sparsity_ratio=sparsity_ratio,\n",
        "    num_calib_rounds=num_calib_rounds,\n",
        "    sparse_ft_epochs=sparse_ft_epochs,\n",
        "    strategy = strategy\n",
        ")\n",
        "\n",
        "# --- Patch et run ---\n",
        "trainer.fit = types.MethodType(fit_with_all_logs, trainer)\n",
        "trainer.fit(n_rounds, eval_fn=eval_fn_test, eval_every=2, checkpoint_path=CHECKPOINT_PATH, resume=True)\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1QUIqffAWaKI"
      },
      "outputs": [],
      "source": [
        "# ====== FL READY: DATA, CLIENTS, WANDB, MODEL EDITING IID TEST ACC ======\n",
        "\n",
        "# --- FL params ---\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 20\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "\n",
        "strategy = \"magnitude_most\"\n",
        "# --- Model Editing HP choosen ---\n",
        "sparsity_ratio = 0.85\n",
        "num_calib_rounds = 1\n",
        "sparse_ft_epochs = 1\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Loads CIFAR-100 brut (no transform here) ---\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- FL split: IID sharding + train local ---\n",
        "client_data = iid_split\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "# ---Instantiate clients prepared for FL (local training only) ---\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    train_idxs = client_data[i]['train']\n",
        "    client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "    clients.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "# --- Official test set ---\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    TransformedSubset(test_set, val_transform), batch_size=128, shuffle=False\n",
        ")\n",
        "\n",
        "# --- Global model ViT-S/16 DINO CIFAR-100 ---\n",
        "global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def eval_fn_test(model):\n",
        "    return evaluate(model, test_loader, device)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path=CHECKPOINT_PATH):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path=CHECKPOINT_PATH):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']})\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def plot_history(acc_history, loss_history, eval_every):\n",
        "    rounds = np.arange(0, len(acc_history))*eval_every + eval_every\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(rounds, acc_history, label='Test Acc')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(rounds, loss_history, label='Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def fit_with_all_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=CHECKPOINT_PATH, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} (MODEL EDITING) ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            test_acc, test_loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Test Acc={test_acc:.3f} | Test Loss={test_loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"test_acc\": test_acc, \"test_loss\": test_loss})\n",
        "            acc_history.append(test_acc)\n",
        "            loss_history.append(test_loss)\n",
        "            if rnd % 5 == 0 or rnd == n_rounds:\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "        if rnd % 5 == 0 or rnd == n_rounds:\n",
        "            plot_history(acc_history, loss_history, eval_every)\n",
        "\n",
        "# --- WANDB init ---\n",
        "wandb.init(\n",
        "    project=\"fl-fedavg-personnal-contribution\",\n",
        "    name=f\"model_editing_iid_test_acc_{strategy}_J{J}_nrounds{n_rounds}_lr{lr}_sp{int(sparsity_ratio*100)}_calib{num_calib_rounds}\",\n",
        "    config={\n",
        "        \"model\": \"DINO ViT-S/16\",\n",
        "        \"K\": K,\n",
        "        \"C\": C,\n",
        "        \"J\": J,\n",
        "        \"n_rounds\": n_rounds,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr,\n",
        "        \"momentum\": momentum,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"sharding\": \"iid\",\n",
        "        \"Nc\": None,\n",
        "        \"use_sparse\": True,\n",
        "        \"sparsity_ratio\": sparsity_ratio,\n",
        "        \"num_calib_rounds\": num_calib_rounds,\n",
        "        \"sparse_ft_epochs\": sparse_ft_epochs,\n",
        "        \"strategy\" : strategy\n",
        "    }\n",
        ")\n",
        "\n",
        "# --- Federated trainer FL-ready ---\n",
        "trainer = FederatedTrainer(\n",
        "    clients=clients,\n",
        "    global_model=global_model,\n",
        "    device=device,\n",
        "    client_fraction=C,\n",
        "    local_epochs=J,\n",
        "    batch_size=batch_size,\n",
        "    lr=lr,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    scheduler_fn=make_scheduler,\n",
        "    use_sparse=True,\n",
        "    sparsity_ratio=sparsity_ratio,\n",
        "    num_calib_rounds=num_calib_rounds,\n",
        "    sparse_ft_epochs=sparse_ft_epochs,\n",
        "    strategy = strategy\n",
        ")\n",
        "\n",
        "# --- Patch et run ---\n",
        "trainer.fit = types.MethodType(fit_with_all_logs, trainer)\n",
        "trainer.fit(n_rounds, eval_fn=eval_fn_test, eval_every=2, checkpoint_path=CHECKPOINT_PATH, resume=True)\n",
        "wandb.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tu63JoUZaRix"
      },
      "outputs": [],
      "source": [
        "# ====== FL READY: DATA, CLIENTS, WANDB, MODEL EDITING IID TEST ACC ======\n",
        "\n",
        "# --- FL params ---\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 20\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "\n",
        "strategy = \"magnitude_least\"\n",
        "# --- Model Editing HP choosen ---\n",
        "sparsity_ratio = 0.90\n",
        "num_calib_rounds = 1\n",
        "sparse_ft_epochs = 1\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Loads CIFAR-100 brut (no transform here) ---\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- FL split: IID sharding + train local ---\n",
        "client_data = iid_split\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "# ---Instantiate clients prepared for FL (local training only) ---\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    train_idxs = client_data[i]['train']\n",
        "    client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "    clients.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "# --- Official test set ---\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    TransformedSubset(test_set, val_transform), batch_size=128, shuffle=False\n",
        ")\n",
        "\n",
        "# --- Global model ViT-S/16 DINO CIFAR-100 ---\n",
        "global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def eval_fn_test(model):\n",
        "    return evaluate(model, test_loader, device)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path=CHECKPOINT_PATH):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path=CHECKPOINT_PATH):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']})\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def plot_history(acc_history, loss_history, eval_every):\n",
        "    rounds = np.arange(0, len(acc_history))*eval_every + eval_every\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(rounds, acc_history, label='Test Acc')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(rounds, loss_history, label='Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def fit_with_all_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=CHECKPOINT_PATH, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} (MODEL EDITING) ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            test_acc, test_loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Test Acc={test_acc:.3f} | Test Loss={test_loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"test_acc\": test_acc, \"test_loss\": test_loss})\n",
        "            acc_history.append(test_acc)\n",
        "            loss_history.append(test_loss)\n",
        "            if rnd % 5 == 0 or rnd == n_rounds:\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "        if rnd % 5 == 0 or rnd == n_rounds:\n",
        "            plot_history(acc_history, loss_history, eval_every)\n",
        "\n",
        "# --- WANDB init ---\n",
        "wandb.init(\n",
        "    project=\"fl-fedavg-personnal-contribution\",\n",
        "    name=f\"model_editing_iid_test_acc_{strategy}_J{J}_nrounds{n_rounds}_lr{lr}_sp{int(sparsity_ratio*100)}_calib{num_calib_rounds}\",\n",
        "    config={\n",
        "        \"model\": \"DINO ViT-S/16\",\n",
        "        \"K\": K,\n",
        "        \"C\": C,\n",
        "        \"J\": J,\n",
        "        \"n_rounds\": n_rounds,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr,\n",
        "        \"momentum\": momentum,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"sharding\": \"iid\",\n",
        "        \"Nc\": None,\n",
        "        \"use_sparse\": True,\n",
        "        \"sparsity_ratio\": sparsity_ratio,\n",
        "        \"num_calib_rounds\": num_calib_rounds,\n",
        "        \"sparse_ft_epochs\": sparse_ft_epochs,\n",
        "        \"strategy\" : strategy\n",
        "    }\n",
        ")\n",
        "\n",
        "# --- Federated trainer FL-ready ---\n",
        "trainer = FederatedTrainer(\n",
        "    clients=clients,\n",
        "    global_model=global_model,\n",
        "    device=device,\n",
        "    client_fraction=C,\n",
        "    local_epochs=J,\n",
        "    batch_size=batch_size,\n",
        "    lr=lr,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    scheduler_fn=make_scheduler,\n",
        "    use_sparse=True,\n",
        "    sparsity_ratio=sparsity_ratio,\n",
        "    num_calib_rounds=num_calib_rounds,\n",
        "    sparse_ft_epochs=sparse_ft_epochs,\n",
        "    strategy = strategy\n",
        ")\n",
        "\n",
        "# --- Patch et run ---\n",
        "trainer.fit = types.MethodType(fit_with_all_logs, trainer)\n",
        "trainer.fit(n_rounds, eval_fn=eval_fn_test, eval_every=2, checkpoint_path=CHECKPOINT_PATH, resume=True)\n",
        "wandb.finish()\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
