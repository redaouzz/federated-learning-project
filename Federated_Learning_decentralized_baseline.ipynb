{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Decentralized baseline"
      ],
      "metadata": {
        "id": "DBdyYw8-u26V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requirements"
      ],
      "metadata": {
        "id": "Z1h6SlJpvGXT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell sets up Weights & Biases to log training metrics and hyperparameters. It’s useful for comparing experiments (e.g. FedAvg vs. model editing) and tracking performance over time."
      ],
      "metadata": {
        "id": "kUVlRjurW4Lq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "9osNaf_Y9EGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Partitioning Strategies for Federated Learning\n",
        "\n",
        "This section defines two key dataset partitioning functions that simulate different data distributions among clients in a Federated Learning (FL) setup using the CIFAR-100 dataset. These functions ensure a realistic simulation of decentralized data scenarios by controlling how training data is split and assigned to each client.\n",
        "\n",
        "### 1. `iid_shard_train_val`\n",
        "This function simulates an i.i.d. (independent and identically distributed) setting by distributing samples equally across clients while maintaining uniform class distribution. Each client receives a balanced subset of the dataset and performs a local train/validation split. This setup mimics a scenario where clients have similar data distributions, which is idealized but useful for baseline comparisons.\n",
        "\n",
        "### 2. `non_iid_shard_train_val`\n",
        "This function implements a non-i.i.d. label-skew sharding strategy, where each client receives data from exactly `Nc` distinct classes without overlap. The data from each class is first divided into shards, which are then randomly distributed to clients such that each one ends up with data belonging to a limited subset of classes. A local train/validation split is also performed. This simulates real-world heterogeneity in FL systems, where clients often observe biased or non-representative data distributions.\n",
        "\n",
        "Together, these two utilities provide a foundational component for studying the impact of data heterogeneity in Federated Learning experiments.\n",
        "\n",
        "This following cell imports all core libraries needed for training: PyTorch, timm for loading pretrained models, torchvision for datasets and transforms, and utilities for data handling and visualization."
      ],
      "metadata": {
        "id": "mjaLzrcyYvwL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import numpy as np\n",
        "from collections import defaultdict, Counter\n",
        "import random\n",
        "import matplotlib.pyplot as plt\n",
        "from typing import List, Dict, Tuple\n"
      ],
      "metadata": {
        "id": "dt3o13zGTmI0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import random_split, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "  \"\"\"\n",
        "  This cell defines the data transformation pipeline and introduces a utility class to apply transformations to subsets of datasets.\n",
        "  Specifically, it creates a TransformedSubset class that wraps around a PyTorch Subset, enabling the application of a custom\n",
        "  transform to the data samples at access time.\n",
        "  \"\"\"\n",
        "\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n"
      ],
      "metadata": {
        "id": "Wp4Qb-UHTo92"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def iid_shard_train_val(dataset, K, val_split=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    This function implements stratified i.i.d. sharding with local train/validation split.\n",
        "\n",
        "    - Ensures each of the K clients receives (approximately) the same number of samples per class.\n",
        "    - Maintains class balance across all clients for fair i.i.d. distribution.\n",
        "    - Performs a local train/validation split within each client, controlled by `val_split`.\n",
        "    - Uses a fixed `seed` to guarantee reproducibility of the sharding.\n",
        "    - Returns a dictionary: {client_id: {'train': [...], 'val': [...]}} mapping client indices to their dataset partitions.\n",
        "    \"\"\"\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
        "    n_classes = len(np.unique(labels))\n",
        "    class_indices = {c: np.where(labels == c)[0] for c in range(n_classes)}\n",
        "    for c in class_indices:\n",
        "        rng.shuffle(class_indices[c])\n",
        "\n",
        "    # Nombre d'exemples par classe à répartir par client\n",
        "    examples_per_class = {c: len(class_indices[c]) // K for c in class_indices}\n",
        "    # On répartit les \"restes\" (si pas divisible) au début\n",
        "    leftovers = {c: len(class_indices[c]) % K for c in class_indices}\n",
        "\n",
        "    client_indices = {i: [] for i in range(K)}\n",
        "    for c in range(n_classes):\n",
        "        idxs = class_indices[c]\n",
        "        cursor = 0\n",
        "        for i in range(K):\n",
        "            take = examples_per_class[c] + (1 if i < leftovers[c] else 0)\n",
        "            client_indices[i].extend(idxs[cursor:cursor+take])\n",
        "            cursor += take\n",
        "\n",
        "    client_data = {}\n",
        "    for i in range(K):\n",
        "        idxs = np.array(client_indices[i])\n",
        "        rng.shuffle(idxs)\n",
        "        n_val = int(len(idxs) * val_split)\n",
        "        val_idxs = idxs[:n_val]\n",
        "        train_idxs = idxs[n_val:]\n",
        "        client_data[i] = {'train': train_idxs.tolist(), 'val': val_idxs.tolist()}\n",
        "    return client_data\n"
      ],
      "metadata": {
        "id": "o_2hMOF4TqqC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def non_iid_shard_train_val(dataset, K, Nc, val_split=0.2, seed=42):\n",
        "    \"\"\"\n",
        "    This function implements non-i.i.d. sharding (label-skew) combined with local train/validation splitting.\n",
        "\n",
        "    - Each of the K clients is assigned Nc **distinct** class shards without class overlap across clients.\n",
        "    - For each class, the examples are partitioned into small shards, which are then randomly distributed to clients.\n",
        "    - The shards ensure that each client sees only a limited and specific subset of classes (controlled by Nc).\n",
        "    - Within each client's shard, a local train/val split is performed according to `val_split`.\n",
        "    - The process is randomized using a fixed `seed` to ensure reproducibility.\n",
        "    - Returns a dictionary: {client_id: {'train': [...], 'val': [...]}} mapping client indices to their dataset splits.\n",
        "    \"\"\"\n",
        "\n",
        "    rng = np.random.RandomState(seed)\n",
        "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
        "    n_classes = len(np.unique(labels))\n",
        "    class_indices = {c: rng.permutation(np.where(labels == c)[0]).tolist() for c in range(n_classes)}\n",
        "    # Générer les shards par classe\n",
        "    shards_per_class = (K * Nc) // n_classes\n",
        "    shards = []\n",
        "    for c in range(n_classes):\n",
        "        idxs = class_indices[c]\n",
        "        shard_size = len(idxs) // shards_per_class\n",
        "        for i in range(shards_per_class):\n",
        "            shard = idxs[i*shard_size:(i+1)*shard_size]\n",
        "            if len(shard) > 0:\n",
        "                shards.append((c, shard))\n",
        "    rng.shuffle(shards)\n",
        "    # Attribution des Nc shards de classes différentes par client\n",
        "    client_shards = {i: [] for i in range(K)}\n",
        "    used = set()\n",
        "    for i in range(K):\n",
        "        chosen = []\n",
        "        class_seen = set()\n",
        "        for s_idx, (c, shard) in enumerate(shards):\n",
        "            if c not in class_seen and s_idx not in used:\n",
        "                chosen.append(s_idx)\n",
        "                class_seen.add(c)\n",
        "            if len(chosen) == Nc:\n",
        "                break\n",
        "        for s_idx in chosen:\n",
        "            used.add(s_idx)\n",
        "            client_shards[i].extend(shards[s_idx][1])\n",
        "    # Split local train/val pour chaque client\n",
        "    client_data = {}\n",
        "    for i in range(K):\n",
        "        idxs = np.array(client_shards[i])\n",
        "        rng.shuffle(idxs)\n",
        "        n_val = int(len(idxs) * val_split)\n",
        "        val_idxs = idxs[:n_val]\n",
        "        train_idxs = idxs[n_val:]\n",
        "        client_data[i] = {'train': train_idxs.tolist(), 'val': val_idxs.tolist()}\n",
        "    return client_data\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "WqGraRlGTs9H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Partitioning and Visualization for Federated Learning\n",
        "\n",
        "This section presents the practical implementation and validation of both i.i.d. and non-i.i.d. data sharding strategies for the CIFAR-100 dataset in the context of Federated Learning (FL). It also includes diagnostic visualizations to assess class distribution across clients.\n",
        "\n",
        "###  i.i.d. Partitioning\n",
        "The CIFAR-100 training set is partitioned into `K = 100` clients using a stratified i.i.d. strategy, ensuring that each client receives a balanced and representative subset of all 100 classes. A local validation set is also carved out for each client. The correctness of the splits is verified by checking for overlapping indices and analyzing global distribution statistics.\n",
        "\n",
        "###  non-i.i.d. Partitioning (Label Skew)\n",
        "In contrast, a non-i.i.d. strategy is used to assign `Nc = 5` unique classes per client without class overlap. This setup reflects real-world data heterogeneity in FL. The resulting partitions are validated in the same way, with additional printouts of class distributions per client to confirm label skew.\n",
        "\n",
        "###  Visualizations\n",
        "Two heatmaps are generated (one for i.i.d. and one for non-i.i.d. sharding) showing the distribution of training samples per class across all clients. In the i.i.d. case, the heatmap shows uniform color intensity, while in the non-i.i.d. case, the matrix is sparse with strong class clustering per client.\n",
        "\n",
        "These steps provide a robust foundation for simulating decentralized data scenarios and are critical to understanding how data heterogeneity impacts model training in Federated Learning.\n"
      ],
      "metadata": {
        "id": "sGoHVoPdlZmX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell loads the CIFAR-100 dataset, performs i.i.d. sharding into K clients, and verifies the correctness of the resulting data splits.\n",
        "\n",
        "- Loads the CIFAR-100 dataset without applying any transformation.\n",
        "- Applies the previously defined `iid_shard_train_val` function to split the training set across K clients with a local train/validation split.\n",
        "- Defines a utility function `check_federated_splits` that validates the integrity of the sharding:\n",
        "  - Ensures no overlap between train and validation sets within a client.\n",
        "  - Ensures no sample is assigned to more than one client.\n",
        "  - Prints distribution statistics such as global train/val ratio and per-client class counts.\n",
        "- This verification helps confirm that the simulated federated dataset is consistent and suitable for experimentation.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "\n",
        "K = 100\n",
        "val_split = 0.2\n",
        "seed = 42\n",
        "\n",
        "client_data = iid_shard_train_val(full_train, K=K, val_split=val_split, seed=seed)\n",
        "\n",
        "def check_federated_splits(dataset, client_data, K, val_split=0.2, mode='iid'):\n",
        "    all_train = []\n",
        "    all_val = []\n",
        "    for i in range(K):\n",
        "        train_idx = client_data[i]['train']\n",
        "        val_idx = client_data[i]['val']\n",
        "        all_train.extend(train_idx)\n",
        "        all_val.extend(val_idx)\n",
        "        # Check overlap train/val local\n",
        "        overlap = set(train_idx) & set(val_idx)\n",
        "        assert len(overlap) == 0, f\"[Client {i}] Overlap train/val!\"\n",
        "    all_indices = all_train + all_val\n",
        "    assert len(all_indices) == len(set(all_indices)),\n",
        "    print(f\"Total samples distributed: {len(all_indices)} / {len(dataset)}\")\n",
        "    n_total = len(dataset)\n",
        "    n_expected_train = int((1-val_split) * n_total / K) * K\n",
        "    n_expected_val = int(val_split * n_total / K) * K\n",
        "    print(f\"Per client (approx.): train={int((1-val_split)*n_total/K)}, val={int(val_split*n_total/K)}\")\n",
        "    print(f\"Mode: {mode.upper()}\")\n",
        "    print(f\"Train/val ratio (global): {len(all_train)/n_total:.3f} / {len(all_val)/n_total:.3f}\")\n",
        "    print(f\"Unique indices: {len(set(all_indices))} (should = {len(all_indices)})\")\n",
        "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
        "    print(f\"Exemple: [Client 0] classes: {set(labels[idx] for idx in client_data[0]['train'])}\")\n",
        "\n",
        "    from collections import Counter\n",
        "    class_counts = Counter([labels[idx] for idx in client_data[0]['train']])\n",
        "    print(f\"[Client 0] Samples per class:\")\n",
        "    for cls in sorted(class_counts.keys()):\n",
        "        print(f\"  Classe {cls}: {class_counts[cls]}\")\n",
        "\n",
        "check_federated_splits(full_train, client_data, K=K, val_split=val_split, mode='iid')\n"
      ],
      "metadata": {
        "id": "Zvt03lRaSM-K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\"\"\"\n",
        "This cell defines and uses a function to visualize the class distribution across clients using a heatmap.\n",
        "\n",
        "- Computes a (K × 100) matrix where each row corresponds to a client and each column to a CIFAR-100 class.\n",
        "- The matrix entries indicate how many training samples of each class are assigned to each client.\n",
        "- Uses seaborn to plot a heatmap showing class imbalance or uniformity across the K clients.\n",
        "- This visualization is useful for verifying the effectiveness of the i.i.d. or non-i.i.d. partitioning strategies.\n",
        "\"\"\"\n",
        "\n",
        "def plot_label_heatmap(dataset, client_data, K):\n",
        "\n",
        "    n_classes = 100  # CIFAR-100\n",
        "    label_matrix = np.zeros((K, n_classes), dtype=int)\n",
        "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
        "\n",
        "    for i in range(K):\n",
        "        idxs = client_data[i]['train']\n",
        "        lbls = labels[idxs]\n",
        "        for c in range(n_classes):\n",
        "            label_matrix[i, c] = np.sum(lbls == c)\n",
        "\n",
        "    plt.figure(figsize=(18, 6))\n",
        "    sns.heatmap(label_matrix, cmap=\"viridis\", annot=False, cbar=True)\n",
        "    plt.xlabel(\"Class\")\n",
        "    plt.ylabel(\"Client\")\n",
        "    plt.title(\"Label distribution per client (train set)\")\n",
        "    plt.show()\n",
        "\n",
        "plot_label_heatmap(full_train, client_data, K)\n"
      ],
      "metadata": {
        "id": "9VBC2AT-VztU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "\"\"\"\n",
        "This cell loads the CIFAR-100 dataset, applies a non-i.i.d. label-skew sharding, and verifies the resulting splits.\n",
        "\n",
        "- Loads the raw CIFAR-100 training dataset (no transformations applied yet).\n",
        "- Uses the `non_iid_shard_train_val` function to distribute data such that each client receives examples from Nc distinct classes.\n",
        "- Performs a local train/validation split per client, with reproducibility ensured by a fixed seed.\n",
        "- Redefines and invokes the `check_federated_splits` function to validate:\n",
        "  - No overlap between training and validation samples.\n",
        "  - No data leakage across clients.\n",
        "  - Global and per-client stats, including class distributions for client 0.\n",
        "- This setup is essential for simulating statistical heterogeneity in Federated Learning.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "\n",
        "K = 100\n",
        "Nc = 5\n",
        "val_split = 0.2\n",
        "seed = 42\n",
        "\n",
        "client_data = non_iid_shard_train_val(full_train, K=K, Nc=Nc, val_split=val_split, seed=seed)\n",
        "\n",
        "def check_federated_splits(dataset, client_data, K, val_split=0.2, mode='non-iid'):\n",
        "    all_train = []\n",
        "    all_val = []\n",
        "    for i in range(K):\n",
        "        train_idx = client_data[i]['train']\n",
        "        val_idx = client_data[i]['val']\n",
        "        all_train.extend(train_idx)\n",
        "        all_val.extend(val_idx)\n",
        "        # Check overlap train/val local\n",
        "        overlap = set(train_idx) & set(val_idx)\n",
        "        assert len(overlap) == 0, f\"[Client {i}] Overlap train/val!\"\n",
        "    all_indices = all_train + all_val\n",
        "    assert len(all_indices) == len(set(all_indices)), \"Some samples are assigned to multiple clients!\"\n",
        "    print(f\"Total samples distributed: {len(all_indices)} / {len(dataset)}\")\n",
        "    n_total = len(dataset)\n",
        "    print(f\"Per client (approx.): train={int((1-val_split)*n_total/K)}, val={int(val_split*n_total/K)}\")\n",
        "    print(f\"Mode: {mode.upper()}\")\n",
        "    print(f\"Train/val ratio (global): {len(all_train)/n_total:.3f} / {len(all_val)/n_total:.3f}\")\n",
        "    print(f\"Unique indices: {len(set(all_indices))} (should = {len(all_indices)})\")\n",
        "    labels = np.array([dataset[i][1] for i in range(len(dataset))])\n",
        "    print(f\"Exemple: [Client 0] classes: {set(labels[idx] for idx in client_data[0]['train'])}\")\n",
        "\n",
        "    from collections import Counter\n",
        "    class_counts = Counter([labels[idx] for idx in client_data[0]['train']])\n",
        "    print(f\"[Client 0] Samples per class:\")\n",
        "    for cls in sorted(class_counts.keys()):\n",
        "        print(f\"  Classe {cls}: {class_counts[cls]}\")\n",
        "\n",
        "check_federated_splits(full_train, client_data, K=K, val_split=val_split, mode='non-iid')\n"
      ],
      "metadata": {
        "id": "SzoTRQG-WXya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import torchvision\n",
        "\n",
        "\"\"\"\n",
        "This cell visualizes the class distribution across clients in a non-i.i.d. label-skew scenario using a heatmap.\n",
        "\n",
        "- Loads the CIFAR-100 training dataset and applies non-i.i.d. sharding with Nc distinct classes per client.\n",
        "- Constructs a matrix of shape (K × 100), where each entry represents the number of training samples of a given class held by a specific client.\n",
        "- Uses seaborn to render a heatmap, clearly illustrating the label imbalance across clients.\n",
        "- This visualization confirms the effectiveness and severity of the non-i.i.d. partitioning and helps identify class sparsity at the client level.\n",
        "\"\"\"\n",
        "\n",
        "K = 100\n",
        "Nc = 5\n",
        "val_split = 0.2\n",
        "seed = 42\n",
        "\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "client_data = non_iid_shard_train_val(full_train, K=K, Nc=Nc, val_split=val_split, seed=seed)\n",
        "\n",
        "labels = np.array([full_train[i][1] for i in range(len(full_train))])\n",
        "\n",
        "mat = np.zeros((K, 100), dtype=int)\n",
        "for i in range(K):\n",
        "    train_idx = client_data[i]['train']\n",
        "    client_labels = labels[train_idx]\n",
        "    for c in range(100):\n",
        "        mat[i, c] = np.sum(client_labels == c)\n",
        "\n",
        "plt.figure(figsize=(20, 7))\n",
        "sns.heatmap(mat, cmap=\"viridis\", cbar=True)\n",
        "plt.xlabel(\"Class\")\n",
        "plt.ylabel(\"Client\")\n",
        "plt.title(\"Label distribution per client (train set, non-iid)\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zfiqROBlW2XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture: ViT-S/16 Pretrained with DINO for CIFAR-100\n",
        "\n",
        "This section defines the neural architecture used throughout the Federated Learning experiments. The model is based on the Vision Transformer (ViT) architecture, specifically the ViT-Small variant with a patch size of 16×16, pretrained using the DINO self-supervised learning framework.\n",
        "\n",
        "Using the `timm` library, the DINO ViT-S/16 model is loaded with its pretrained weights. The final DINO classification head is removed and replaced by a custom linear classifier with 100 output units, corresponding to the 100 classes of the CIFAR-100 dataset. The feature extractor produces 384-dimensional embeddings, which are passed to the classifier for supervised learning.\n",
        "\n",
        "This lightweight yet expressive architecture serves as the base model for both centralized training and Federated Learning settings, and will later be used in combination with sparse fine-tuning and model editing techniques.\n"
      ],
      "metadata": {
        "id": "1tOlOzjDc7ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import timm\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "\"\"\"\n",
        "This cell defines the neural network architecture used for classification: a modified ViT-S/16 model pre-trained with DINO.\n",
        "\n",
        "- Leverages the `timm` library to load a pretrained ViT-Small (patch size 16) model trained with DINO self-supervision.\n",
        "- Removes the DINO head (`nn.Identity`) to extract raw features from the backbone.\n",
        "- Adds a custom linear classifier (`nn.Linear`) to adapt the model for the CIFAR-100 classification task (100 output classes).\n",
        "- The resulting architecture combines strong pre-trained representations with a lightweight classifier suitable for fine-tuning or sparse updates.\n",
        "\"\"\"\n",
        "\n",
        "class DinoViT_CIFAR100(nn.Module):\n",
        "    def __init__(self, num_classes=100):\n",
        "        super().__init__()\n",
        "        self.backbone = timm.create_model('vit_small_patch16_224.dino', pretrained=True)\n",
        "        # the dimension of features for ViT-S/16 is always 384 (see doc timm/models/vit.py)\n",
        "        self.backbone.head = nn.Identity()  # takes off head DINO\n",
        "        self.classifier = nn.Linear(384, num_classes)\n",
        "    def forward(self, x):\n",
        "        # timm ViT returns (batch, 384) if the head is nn.Identity\n",
        "        feats = self.backbone(x)   # (batch, 384)\n",
        "        out = self.classifier(feats)\n",
        "        return out\n",
        "\n"
      ],
      "metadata": {
        "id": "7ojiqaF5WFHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sparse Fine-Tuning and Model Editing with Fisher-Based Masking\n",
        "\n",
        "This section introduces the core components enabling sparse fine-tuning for model editing in a Federated Learning context. The approach leverages Fisher Information to identify and selectively update the most relevant parameters, reducing interference and improving communication efficiency.\n",
        "\n",
        "###  Fisher Score Approximation\n",
        "\n",
        "The function `_compute_approximated_fisher_scores` estimates the diagonal of the Fisher Information Matrix by accumulating squared gradients over multiple batches. This approximation identifies parameter sensitivity, with low-scoring parameters considered less critical. These scores serve as the foundation for constructing binary masks that govern which weights are updated during fine-tuning. Additional utilities such as `_num_total_params`, `_num_zero_params`, and `_compute_sparsity` allow monitoring the proportion of masked parameters and evaluating sparsity levels.\n",
        "\n",
        "### Progressive Gradient Mask Calibration\n",
        "\n",
        "`calibrate_gradient_mask_progressive` progressively builds a binary gradient mask over several rounds by freezing the least important weights identified via Fisher scores. At each step, a smaller fraction of parameters is retained according to a decreasing keep ratio, gradually refining the mask until the target sparsity is reached. This method supports only the \"train_least_important\" strategy in its current implementation.\n",
        "\n",
        "### Sparse Optimizer with Gradient Masking\n",
        "\n",
        "The custom optimizer `SparseSGDM` extends PyTorch's SGD to incorporate gradient masking. During each optimization step, gradients for masked parameters are explicitly zeroed out using the precomputed binary masks. The optimizer maps each parameter to its corresponding mask using unique identifiers, ensuring selective updates.\n",
        "\n",
        "### Sparse Fine-Tuning Procedure\n",
        "\n",
        "`sparse_fine_tune` orchestrates the fine-tuning loop by applying the gradient mask and updating only the unmasked parameters. It adjusts the `requires_grad` flag before training and restores it afterward to avoid side effects. Combined with `SparseSGDM`, it ensures that the model undergoes low-interference updates consistent with the calibrated mask, enabling efficient and targeted model editing.\n",
        "\n",
        "These tools together provide a robust framework for exploring model editing in federated settings, allowing flexible control over parameter updates and enabling experimentation with various sparsity and sensitivity-driven strategies.\n"
      ],
      "metadata": {
        "id": "9V7dPxRRoNAj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "This cell defines helper functions for sparse mask analysis and for computing approximated Fisher scores.\n",
        "\n",
        "- `_num_total_params`, `_num_zero_params`, and `_compute_sparsity`:\n",
        "  - Provide statistics on a binary mask by calculating the total number of parameters, how many are zeroed (masked), and the resulting sparsity ratio.\n",
        "\n",
        "- `_compute_approximated_fisher_scores`:\n",
        "  - Estimates the diagonal of the Fisher Information Matrix using squared gradients averaged over a number of batches.\n",
        "  - Operates on a validation or local client dataloader.\n",
        "  - Supports optional masking to compute scores only over active (unmasked) parameters.\n",
        "  - Returns a dictionary `{param_name: Fisher_diag_tensor}` to be used for identifying low-sensitivity weights during mask calibration.\n",
        "\n",
        "These functions form the core utilities behind sensitivity-based sparse model editing in Federated Learning.\n",
        "\"\"\"\n",
        "\n",
        "def _num_total_params(mask: Dict[str, torch.Tensor]) -> int:\n",
        "    \"\"\"Returns the total number of parameters (elements) across all tensors in the mask.\"\"\"\n",
        "    return sum(t.numel() for t in mask.values())\n",
        "\n",
        "def _num_zero_params(mask: Dict[str, torch.Tensor]) -> int:\n",
        "    \"\"\"Returns the number of parameters set to zero in the mask (i.e., masked out).\"\"\"\n",
        "    return sum((t == 0).sum().item() for t in mask.values())\n",
        "\n",
        "def _compute_sparsity(mask: Dict[str, torch.Tensor]) -> float:\n",
        "    \"\"\"Returns the sparsity, i.e., the fraction of parameters that are masked (value in [0, 1]).\"\"\"\n",
        "    return _num_zero_params(mask) / _num_total_params(mask)\n",
        "\n",
        "\n",
        "def _compute_approximated_fisher_scores(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    loss_fn: nn.Module,\n",
        "    device: torch.device,\n",
        "    num_batches: Optional[int] = None,\n",
        "    mask: Optional[Dict[str, torch.Tensor]] = None\n",
        "):\n",
        "    \"\"\"\n",
        "    Approximate the diagonal of the Fisher Information Matrix via empirical average.\n",
        "    Args:\n",
        "        model: torch.nn.Module\n",
        "        dataloader: DataLoader (local client data)\n",
        "        loss_fn: torch.nn loss function (e.g. nn.CrossEntropyLoss())\n",
        "        device: torch.device\n",
        "        num_batches: number of batches to use for approximation\n",
        "    Returns:\n",
        "        Dict {param_name: tensor of Fisher diagonal}\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    fisher_diag = {\n",
        "        name: torch.zeros_like(param, device=device)\n",
        "        for name, param in model.named_parameters()\n",
        "        if param.requires_grad\n",
        "    }\n",
        "    total_batches = len(dataloader) if num_batches is None else num_batches\n",
        "\n",
        "    for batch_idx, (inputs, targets) in enumerate(\n",
        "        tqdm(dataloader, total=total_batches, desc=\"Computing Fisher\")\n",
        "    ):\n",
        "        if num_batches is not None and batch_idx >= num_batches:\n",
        "            break\n",
        "\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        model.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_fn(outputs, targets)\n",
        "        loss.backward()\n",
        "\n",
        "        for name, param in model.named_parameters():\n",
        "            if param.grad is not None:\n",
        "                fisher_diag[name] += param.grad.detach() ** 2\n",
        "                if mask is not None:\n",
        "                    fisher_diag[name] *= mask[name]\n",
        "\n",
        "    for name in fisher_diag:\n",
        "        fisher_diag[name] /= total_batches\n",
        "\n",
        "    return fisher_diag\n"
      ],
      "metadata": {
        "id": "qXzvAUEdVFdT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def calibrate_gradient_mask_progressive(\n",
        "    model: nn.Module,\n",
        "    dataloader: DataLoader,\n",
        "    device: torch.device,\n",
        "    sparsity: float = 0.9,\n",
        "    rounds: int = 5,\n",
        "    num_batches: Optional[int] = None,\n",
        "    loss_fn: nn.Module = nn.CrossEntropyLoss(),\n",
        "    approximate_fisher: bool = True,\n",
        "    strategy: str = \"train_least_important\",\n",
        ") -> Dict[str, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    This function performs progressive gradient mask calibration using approximate Fisher Information scores.\n",
        "\n",
        "    - Supports the \"train_least_important\" strategy, which iteratively identifies and freezes the least sensitive parameters.\n",
        "    - At each round, scores are computed (currently only approximate Fisher scores), and the active parameter set is pruned further by updating a binary mask.\n",
        "    - The fraction of parameters retained decreases progressively over multiple calibration rounds to reach the desired sparsity level.\n",
        "    - The function maintains and updates a dictionary of binary masks (`{param_name: mask}`), which can later be used to freeze selected weights during fine-tuning.\n",
        "    - Useful for implementing sparse fine-tuning strategies in Federated Learning or model editing without retraining the entire model.\n",
        "    \"\"\"\n",
        "\n",
        "    print(\"*\" * 50)\n",
        "    print(f\"Progressive Mask Calibration - Strategy: {strategy}\")\n",
        "    print(\"*\" * 50)\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    mask = {\n",
        "        n: torch.ones_like(p, device=device)\n",
        "        for n, p in model.named_parameters()\n",
        "        if p.requires_grad\n",
        "    }\n",
        "\n",
        "    for r in range(1, rounds + 1):\n",
        "        print(f\"[Round {r}]\")\n",
        "\n",
        "        # Score computation (only approximate Fisher supported here)\n",
        "        if approximate_fisher:\n",
        "            scores = _compute_approximated_fisher_scores(\n",
        "                model=model,\n",
        "                dataloader=dataloader,\n",
        "                loss_fn=loss_fn,\n",
        "                num_batches=num_batches,\n",
        "                device=device,\n",
        "                mask=mask,\n",
        "            )\n",
        "        else:\n",
        "            raise NotImplementedError(\"Only approximate Fisher is implemented.\")\n",
        "\n",
        "         # 1. take every scores (to log, debug)\n",
        "        all_scores = torch.cat([v.flatten() for v in scores.values()])\n",
        "        # 2. Retain only the scores of parameters that remain active (i.e., where mask == 1)\n",
        "        active_scores = torch.cat([\n",
        "            score[mask[name] != 0].flatten()\n",
        "            for name, score in scores.items()\n",
        "        ])\n",
        "        total_params = all_scores.numel()\n",
        "        total_active_params = active_scores.numel()\n",
        "\n",
        "        # Exponentially decrease keep_fraction for progressive pruning\n",
        "        keep_fraction = (1-sparsity) ** (r / rounds)\n",
        "        k = int(keep_fraction * total_params)\n",
        "        print(f\"Current keep fraction: {keep_fraction:.4f} | Keeping only top k: {k}\")\n",
        "\n",
        "        if strategy == \"train_least_important\":\n",
        "            #To prevent bugs: ensure that k does not exceed the number of active parameters\n",
        "            k = max(1, min(k, total_active_params))\n",
        "            threshold, _ = torch.kthvalue(active_scores, k)\n",
        "            print(\"Threshold (below which params are kept):\", threshold)\n",
        "            for name, score in scores.items():\n",
        "                # Mask only newly selected parameters; keep previously zeroed (masked) ones unchanged\n",
        "                new_mask = (score <= threshold).float()\n",
        "                mask[name] = mask[name] * new_mask\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "\n",
        "        print(\n",
        "            f\"After round {r} mask sparsity: { _compute_sparsity(mask):.4f} \"\n",
        "            f\"({_num_zero_params(mask)}/{_num_total_params(mask)} zeroed params)\"\n",
        "        )\n",
        "        print()\n",
        "\n",
        "    print(\"Progressive Mask Calibration completed.\")\n",
        "    return mask\n"
      ],
      "metadata": {
        "id": "1FzqXm9oVG8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.optim import SGD\n",
        "from typing import Dict, Iterable\n",
        "\n",
        "\"\"\"\n",
        "This cell defines `SparseSGDM`, a custom optimizer that extends PyTorch's SGD to support gradient masking.\n",
        "\n",
        "- Inherits from `torch.optim.SGD` and adds support for per-parameter binary masks.\n",
        "- During each optimization step, gradients of masked-out parameters are zeroed out before the update.\n",
        "- Accepts both the list of parameters (`params`) and their associated names (`named_params`) to align each gradient with its corresponding mask.\n",
        "- This mechanism enables sparse fine-tuning by ensuring that only a specific subset of parameters (those with mask=1) are updated.\n",
        "- Especially useful in Federated Learning and model editing where memory and compute constraints require sparse updates.\n",
        "\"\"\"\n",
        "\n",
        "class SparseSGDM(SGD):\n",
        "    def __init__(\n",
        "        self,\n",
        "        params: Iterable[torch.nn.Parameter],\n",
        "        named_params: Dict[str, torch.nn.Parameter],\n",
        "        lr: float,\n",
        "        momentum: float = 0.0,\n",
        "        weight_decay: float = 0.0,\n",
        "        mask: Dict[str, torch.Tensor] = None,\n",
        "    ):\n",
        "        super().__init__(\n",
        "            params,\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            weight_decay=weight_decay,\n",
        "        )\n",
        "        self.mask = mask  # Dict {param_name: mask_tensor}\n",
        "        self.named_params = named_params  # Dict {name: param}\n",
        "        self.param_id_to_name = {id(p): n for n, p in named_params.items()}\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def step(self, closure=None):\n",
        "        if closure is not None:\n",
        "            with torch.enable_grad():\n",
        "                closure()\n",
        "\n",
        "        for group in self.param_groups:\n",
        "            for p in group[\"params\"]:\n",
        "                name = self.param_id_to_name.get(id(p))\n",
        "                if p.grad is not None and self.mask is not None and name in self.mask:\n",
        "                    p.grad.mul_(self.mask[name])\n",
        "\n",
        "        return super().step(closure)\n"
      ],
      "metadata": {
        "id": "IpSM-leqVJjA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def sparse_fine_tune(\n",
        "    model: nn.Module,\n",
        "    dataloader,\n",
        "    device,\n",
        "    mask,\n",
        "    lr=1e-3,\n",
        "    epochs=1,\n",
        "    momentum=0.9,\n",
        "    weight_decay=5e-4,\n",
        "):\n",
        "    \"\"\"\n",
        "    This function performs sparse fine-tuning of a model using a fixed binary mask.\n",
        "\n",
        "    - Freezes all parameters except those marked with 1 in the provided mask, by setting `requires_grad` accordingly.\n",
        "    - Uses the `SparseSGDM` optimizer to ensure that masked gradients are zeroed out during training.\n",
        "    - Runs standard training for a given number of epochs using cross-entropy loss on the specified dataloader.\n",
        "    - Only the subset of parameters defined by the mask are updated; all others remain unchanged throughout fine-tuning.\n",
        "    - After training, it resets all parameters’ `requires_grad` flags to True to avoid unintended side effects if the model is reused.\n",
        "    - This approach is central to efficient sparse model editing in Federated Learning and continual learning contexts.\n",
        "    \"\"\"\n",
        "\n",
        "    model.to(device)\n",
        "    # Set requires_grad according to the mask\n",
        "    for name, param in model.named_parameters():\n",
        "        if name in mask:\n",
        "            param.requires_grad = (mask[name] == 1).any().item()\n",
        "        else:\n",
        "            param.requires_grad = False\n",
        "\n",
        "    # Prepare params to optimize (only those with requires_grad)\n",
        "    named_params = dict(model.named_parameters())\n",
        "    params = [p for p  in named_params.values() if p.requires_grad]\n",
        "\n",
        "    # SGD standard (no need for custom optimizer since masking is done by requires_grad)\n",
        "    optimizer = SparseSGDM(\n",
        "        params=params,\n",
        "        named_params=named_params,\n",
        "        lr=lr,\n",
        "        momentum=momentum,\n",
        "        weight_decay=weight_decay,\n",
        "        mask=mask,\n",
        "    )\n",
        "\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        for inputs, targets in dataloader:\n",
        "            inputs, targets = inputs.to(device), targets.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, targets)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Optional: reset requires_grad to True for all params if you reuse model elsewhere\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = True\n",
        "\n"
      ],
      "metadata": {
        "id": "dnl1bm74VM6z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federated Orchestration: Client Logic, Server Aggregation, and Evaluation\n",
        "\n",
        "This section implements the full Federated Learning pipeline, including the local client behavior, the central training loop (FedAvg), and global model evaluation. It supports both standard and sparse fine-tuning regimes, enabling experimentation with model editing in a decentralized setting.\n",
        "\n",
        "### `Client` Class\n",
        "\n",
        "Each client represents a participant in the Federated Learning setup and encapsulates all logic for local training. Clients hold their own private dataset and device context, and can perform:\n",
        "\n",
        "- **Standard training**, where all parameters are updated using SGD.\n",
        "- **Sparse fine-tuning**, where only a subset of parameters—selected via a precomputed binary mask—are updated.\n",
        "- **Mask calibration**, using Fisher Information to identify the least important parameters and construct a sparsity-aware update mask.\n",
        "\n",
        "This design makes each client modular and self-contained, allowing seamless switching between dense and sparse update strategies.\n",
        "\n",
        "### `FederatedTrainer` Class\n",
        "\n",
        "This class orchestrates the full FL loop, using the FedAvg algorithm:\n",
        "\n",
        "- At each round, a random subset of clients is selected according to a specified participation rate.\n",
        "- Each client performs local training (either full or sparse, depending on the `use_sparse` flag).\n",
        "- The server aggregates the clients’ model weights using a sample-weighted average to update the global model.\n",
        "- Optional evaluation can be performed after each round using a provided evaluation function.\n",
        "\n",
        "The trainer supports non-i.i.d. settings and sparse model editing strategies, making it highly configurable for a variety of FL experiments.\n",
        "\n",
        "### `evaluate` Function\n",
        "\n",
        "A utility function to assess model performance on a validation or test set. It computes classification accuracy and average loss over a given dataloader. This function is used during federated training to track model convergence and generalization.\n",
        "\n",
        "Together, these components enable a complete and flexible Federated Learning pipeline that integrates sparse fine-tuning techniques, offering scalability, efficiency, and adaptability to real-world data heterogeneity.\n"
      ],
      "metadata": {
        "id": "CbaXxCXnrfPH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Client:\n",
        "    \"\"\"\n",
        "    This class defines a `Client` in a Federated Learning setting, capable of performing both standard local training\n",
        "    and sparse fine-tuning using a binary mask.\n",
        "\n",
        "    - Each client holds a private dataset and its own device context for local computation.\n",
        "    - `calibrate_mask` computes a binary gradient mask using Fisher scores (or other strategies) to identify the least important parameters.\n",
        "    - `apply_mask_requires_grad` sets the `requires_grad` flag based on the mask, effectively freezing non-selected weights.\n",
        "    - `sparse_fine_tune` fine-tunes only the unmasked parameters using SGD, keeping others fixed during training.\n",
        "    - `local_train` is the main orchestration method:\n",
        "      - If `use_sparse` is `True`, it triggers mask calibration and sparse fine-tuning for a few epochs.\n",
        "      - Otherwise, it performs standard local training using all parameters with optional scheduler support.\n",
        "    - This class encapsulates all logic needed for local updates in a Federated Learning loop, supporting experimentation with model editing techniques.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, client_id, dataset, device):\n",
        "        self.client_id = client_id\n",
        "        self.dataset = dataset\n",
        "        self.device = device\n",
        "        self.last_mask = None  # Store the mask if needed\n",
        "\n",
        "    def calibrate_mask(\n",
        "        self,\n",
        "        model,\n",
        "        sparsity_ratio=0.9,\n",
        "        num_calib_rounds=5,\n",
        "        batch_size=128,\n",
        "        num_batches: Optional[int] = None,\n",
        "        loss_fn=None,\n",
        "        strategy: str = \"train_least_important\",\n",
        "    ):\n",
        "        \"\"\"Calibrate a binary mask based on importance strategy (Fisher, magnitude, or random).\"\"\"\n",
        "        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "        if loss_fn is None:\n",
        "            loss_fn = nn.CrossEntropyLoss()\n",
        "        mask = calibrate_gradient_mask_progressive(\n",
        "            model=model,\n",
        "            dataloader=dataloader,\n",
        "            device=self.device,\n",
        "            sparsity=sparsity_ratio,\n",
        "            rounds=num_calib_rounds,\n",
        "            num_batches=num_batches,\n",
        "            loss_fn=loss_fn,\n",
        "            approximate_fisher=True,\n",
        "            strategy=strategy,\n",
        "        )\n",
        "        self.last_mask = mask\n",
        "        return mask\n",
        "\n",
        "    def apply_mask_requires_grad(self, model, mask):\n",
        "        \"\"\"\n",
        "        Sets requires_grad=True for params where mask == 1, False otherwise.\n",
        "        \"\"\"\n",
        "        for name, param in model.named_parameters():\n",
        "            if name in mask:\n",
        "                param.requires_grad = (mask[name] == 1).any().item()\n",
        "            else:\n",
        "                param.requires_grad = False\n",
        "\n",
        "    def sparse_fine_tune(\n",
        "        self,\n",
        "        model,\n",
        "        mask,\n",
        "        lr=1e-3,\n",
        "        epochs=1,\n",
        "        batch_size=128,\n",
        "        momentum=0.9,\n",
        "        weight_decay=5e-4,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Sparse fine-tuning: only params with requires_grad=True (i.e. mask==1) are updated.\n",
        "        \"\"\"\n",
        "        self.apply_mask_requires_grad(model, mask)\n",
        "        dataloader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "        optimizer = torch.optim.SGD(\n",
        "            filter(lambda p: p.requires_grad, model.parameters()),\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            weight_decay=weight_decay,\n",
        "        )\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "        model.train()\n",
        "        for epoch in range(epochs):\n",
        "            for inputs, targets in dataloader:\n",
        "                inputs, targets = inputs.to(self.device), targets.to(self.device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, targets)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "        # Reset requires_grad\n",
        "        for param in model.parameters():\n",
        "            param.requires_grad = True\n",
        "\n",
        "    def local_train(\n",
        "        self,\n",
        "        global_model,\n",
        "        epochs,\n",
        "        batch_size,\n",
        "        lr,\n",
        "        momentum,\n",
        "        weight_decay,\n",
        "        scheduler_fn=None,\n",
        "        use_sparse=False,\n",
        "        sparsity_ratio=0.9,\n",
        "        num_calib_rounds=5,\n",
        "        num_batches: Optional[int] = None,\n",
        "        sparse_ft_epochs=1,\n",
        "        strategy: str = \"train_least_important\",\n",
        "    ):\n",
        "        \"\"\"\n",
        "        Performs standard local training or (if use_sparse) sparse fine-tuning.\n",
        "        \"\"\"\n",
        "        model = DinoViT_CIFAR100(num_classes=100).to(self.device)\n",
        "        model.load_state_dict(global_model.state_dict())\n",
        "\n",
        "        if use_sparse:\n",
        "            mask = self.calibrate_mask(\n",
        "                model,\n",
        "                sparsity_ratio=sparsity_ratio,\n",
        "                num_calib_rounds=num_calib_rounds,\n",
        "                batch_size=batch_size,\n",
        "                num_batches=num_batches,\n",
        "                strategy=strategy,  # ← AJOUT\n",
        "            )\n",
        "            self.sparse_fine_tune(\n",
        "                model,\n",
        "                mask,\n",
        "                lr=lr,\n",
        "                epochs=sparse_ft_epochs,\n",
        "                batch_size=batch_size,\n",
        "                momentum=momentum,\n",
        "                weight_decay=weight_decay,\n",
        "            )\n",
        "        else:\n",
        "            # Standard local training\n",
        "            loader = DataLoader(self.dataset, batch_size=batch_size, shuffle=True)\n",
        "            optimizer = torch.optim.SGD(\n",
        "                filter(lambda p: p.requires_grad, model.parameters()),\n",
        "                lr=lr,\n",
        "                momentum=momentum,\n",
        "                weight_decay=weight_decay,\n",
        "            )\n",
        "            scheduler = scheduler_fn(optimizer) if scheduler_fn else None\n",
        "            criterion = nn.CrossEntropyLoss()\n",
        "            model.train()\n",
        "            for epoch in range(epochs):\n",
        "                for X, y in loader:\n",
        "                    X, y = X.to(self.device), y.to(self.device)\n",
        "                    optimizer.zero_grad()\n",
        "                    loss = criterion(model(X), y)\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                if scheduler:\n",
        "                    scheduler.step()\n",
        "\n",
        "        return model.state_dict()\n"
      ],
      "metadata": {
        "id": "JjHMwIqdTzwJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FederatedTrainer:\n",
        "    \"\"\"\n",
        "    This class defines a `FederatedTrainer` that orchestrates the entire Federated Learning process using FedAvg,\n",
        "    with optional support for sparse model editing.\n",
        "\n",
        "    - The constructor initializes hyperparameters, the global model, and flags for enabling sparse fine-tuning strategies.\n",
        "    - `aggregate_weights` performs FedAvg-style aggregation by computing a sample-weighted average of client models.\n",
        "    - `train_round` executes one communication round:\n",
        "      - Randomly selects a fraction of clients.\n",
        "      - Each selected client performs either standard training or sparse fine-tuning, based on the `use_sparse` flag.\n",
        "      - Their resulting model weights and dataset sizes are collected for weighted aggregation.\n",
        "    - `fit` coordinates multiple training rounds, optionally evaluating the global model every `eval_every` rounds using a user-provided `eval_fn`.\n",
        "\n",
        "    This class encapsulates both standard and sparsity-aware federated training workflows, making it a flexible engine for experimentation in FL settings.\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        clients,\n",
        "        global_model,\n",
        "        device,\n",
        "        client_fraction,\n",
        "        local_epochs,\n",
        "        batch_size,\n",
        "        lr,\n",
        "        momentum,\n",
        "        weight_decay,\n",
        "        scheduler_fn=None,\n",
        "        use_sparse=False,\n",
        "        sparsity_ratio=0.9,\n",
        "        num_calib_rounds=5,\n",
        "        num_batches: Optional[int] = None,\n",
        "        sparse_ft_epochs=1,\n",
        "        strategy: str = \"train_least_important\",\n",
        "    ):\n",
        "        self.clients = clients\n",
        "        self.global_model = global_model\n",
        "        self.device = device\n",
        "        self.client_fraction = client_fraction\n",
        "        self.local_epochs = local_epochs\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.momentum = momentum\n",
        "        self.weight_decay = weight_decay\n",
        "        self.scheduler_fn = scheduler_fn\n",
        "\n",
        "        self.use_sparse = use_sparse\n",
        "        self.sparsity_ratio = sparsity_ratio\n",
        "        self.num_calib_rounds = num_calib_rounds\n",
        "        self.num_batches = num_batches\n",
        "        self.sparse_ft_epochs = sparse_ft_epochs\n",
        "        self.strategy = strategy\n",
        "\n",
        "    def aggregate_weights(self, client_states, client_sizes):\n",
        "        \"\"\"\n",
        "        Weighted average (FedAvg) of the selected client weights.\n",
        "        client_states: list of state_dicts (one per client)\n",
        "        client_sizes: list of int (number of samples per client)\n",
        "        \"\"\"\n",
        "        total = sum(client_sizes)\n",
        "        avg_state = {}\n",
        "        for key in client_states[0].keys():\n",
        "            weighted_sum = sum(state[key].float() * size for state, size in zip(client_states, client_sizes))\n",
        "            avg_state[key] = weighted_sum / total\n",
        "        return avg_state\n",
        "\n",
        "    def train_round(self):\n",
        "        \"\"\"\n",
        "        Runs one FedAvg round with optional model editing (sparse fine-tune).\n",
        "        Aggregates using sample-weighted mean (FedAvg-style).\n",
        "        \"\"\"\n",
        "        num_clients = len(self.clients)\n",
        "        m = max(int(self.client_fraction * num_clients), 1)\n",
        "        selected = np.random.choice(self.clients, m, replace=False)\n",
        "        client_states = []\n",
        "        client_sizes = []\n",
        "\n",
        "        for client in selected:\n",
        "            client_state = client.local_train(\n",
        "                global_model=self.global_model,\n",
        "                epochs=self.local_epochs,\n",
        "                batch_size=self.batch_size,\n",
        "                lr=self.lr,\n",
        "                momentum=self.momentum,\n",
        "                weight_decay=self.weight_decay,\n",
        "                scheduler_fn=self.scheduler_fn,\n",
        "                use_sparse=self.use_sparse,\n",
        "                sparsity_ratio=self.sparsity_ratio,\n",
        "                num_calib_rounds=self.num_calib_rounds,\n",
        "                num_batches=self.num_batches,\n",
        "                sparse_ft_epochs=self.sparse_ft_epochs,\n",
        "                strategy=self.strategy  # ← AJOUT\n",
        "            )\n",
        "            client_states.append(client_state)\n",
        "            client_sizes.append(len(client.dataset))\n",
        "\n",
        "        avg_state = self.aggregate_weights(client_states, client_sizes)\n",
        "        self.global_model.load_state_dict(avg_state)\n",
        "\n",
        "    def fit(self, n_rounds, eval_fn=None, eval_every=1):\n",
        "        for rnd in range(1, n_rounds + 1):\n",
        "            print(f'---- FedAvg Round {rnd} {\"(SPARSE-EDITING)\" if self.use_sparse else \"\"} ----')\n",
        "            self.train_round()\n",
        "            if eval_fn and (rnd % eval_every == 0 or rnd == n_rounds):\n",
        "                acc, loss = eval_fn(self.global_model)\n",
        "                print(f'[Round {rnd}] Eval: Acc={acc:.3f} | Loss={loss:.3f}')\n"
      ],
      "metadata": {
        "id": "H-Ty52mIT0rM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def evaluate(model, dataloader, device):\n",
        "    \"\"\"\n",
        "    This function evaluates a classification model on a given dataset.\n",
        "\n",
        "    - Runs inference on the provided `dataloader` using `torch.no_grad()` to disable gradient tracking.\n",
        "    - Computes both the total number of correct predictions and the average cross-entropy loss.\n",
        "    - Returns a tuple `(accuracy, average_loss)` which can be used to track model performance over time.\n",
        "    - Used during federated training (e.g., in the `fit` method of `FederatedTrainer`) to evaluate the global model at regular intervals.\n",
        "    \"\"\"\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    correct, total, total_loss = 0, 0, 0.0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            outputs = model(X)\n",
        "            loss = criterion(outputs, y)\n",
        "            total_loss += loss.item() * X.size(0)\n",
        "            _, preds = outputs.max(1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += X.size(0)\n",
        "    if total == 0:\n",
        "        return 0.0, 0.0\n",
        "    return correct / total, total_loss / total\n"
      ],
      "metadata": {
        "id": "lzYN1EX0T2m5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset Sharding Setup: IID and Non-IID Splits\n",
        "\n",
        "This section initializes the dataset and prepares the client-specific data splits for all future Federated Learning experiments.\n",
        "\n",
        "- The CIFAR-100 dataset is loaded in its raw form, without transformations.\n",
        "- Two types of data partitioning are applied to the training set:\n",
        "  - **IID Split**: Each of the 100 clients receives a balanced subset of the dataset, covering all classes uniformly.\n",
        "  - **Non-IID Split**: Each client receives examples from a limited number of classes (`Nc`, e.g., 50), simulating label skew and statistical heterogeneity.\n",
        "- The resulting splits (`iid_split` and `non_iid_split`) include both training and validation indices for every client.\n",
        "- These partitions are fixed and reusable across all runs, ensuring consistent conditions for evaluating different models and strategies.\n"
      ],
      "metadata": {
        "id": "4Ynidmb3r6Fi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparams\n",
        "K = 100\n",
        "val_split = 0.2\n",
        "seed = 42\n",
        "Nc = 50\n",
        "\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "iid_split = iid_shard_train_val(full_train, K=K, val_split=val_split, seed=seed)\n",
        "\n",
        "non_iid_split = non_iid_shard_train_val(full_train, K=K, Nc=Nc, val_split=val_split, seed=seed)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Dvv3Dq1sNgd5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FedAvg IID"
      ],
      "metadata": {
        "id": "K539desWuEYd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell launches a complete Federated Learning experiment using the FedAvg algorithm on the CIFAR-100 dataset with 100 i.i.d. clients and a DINO ViT-S/16 model. It sets the main hyperparameters, initializes Weights & Biases for experiment tracking, and handles training, evaluation, checkpointing, and plotting. The fit method of the trainer is overridden to incorporate all these features, allowing the experiment to be resumed seamlessly and the best-performing configuration to be saved for further analysis."
      ],
      "metadata": {
        "id": "9drVx3Hwh0nj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federated Training with FedAvg and Grid Search\n",
        "\n",
        "This cell sets up and launches a full Federated Learning experiment using the FedAvg algorithm on the CIFAR-100 dataset with ViT-S/16 (pretrained with DINO). The training is performed under an i.i.d. data distribution across 100 clients and includes grid search over key hyperparameters.\n",
        "\n",
        "### Experiment Setup\n",
        "- **Dataset**: CIFAR-100 is partitioned using an i.i.d. strategy with local train/validation splits.\n",
        "- **Model**: A DINO-pretrained ViT-S/16 model adapted for CIFAR-100 classification.\n",
        "- **Clients**: Each client is assigned a local dataset and instantiated with transformation pipelines.\n",
        "- **Global validation**: A centralized validation set is constructed by aggregating all local validation samples.\n",
        "\n",
        "### Hyperparameter Grid Search\n",
        "- A grid search is performed over combinations of learning rates (`lr_list`) and momentum values (`momentum_list`).\n",
        "- For each configuration, a new global model and `FederatedTrainer` instance is initialized.\n",
        "\n",
        "### Training Logic\n",
        "- The trainer runs `n_rounds` of FedAvg with a client fraction `C`.\n",
        "- After every few rounds, validation performance is evaluated and logged to Weights & Biases (wandb).\n",
        "- Results, including best validation accuracy and configuration metadata, are saved to disk in JSON format.\n",
        "- A custom `fit_with_all_logs` method is used to support checkpointing, live plotting, and structured logging.\n",
        "\n",
        "This workflow enables large-scale, reproducible Federated Learning experiments with robust logging, checkpointing, and evaluation, serving as a baseline for future comparisons with sparse model editing or non-i.i.d. setups.\n"
      ],
      "metadata": {
        "id": "cB-pLPRropwY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import wandb\n",
        "import types\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- FL params ---\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 20\n",
        "batch_size = 128\n",
        "weight_decay = 5e-4\n",
        "\n",
        "# --- Hyperparams ---\n",
        "lr_list = [0.001, 0.01, 0.1]\n",
        "momentum_list = [0.8, 0.9, 0.95]\n",
        "\n",
        "CHECKPOINT_PATH = \"fedavg_grid_ckpt.pt\"\n",
        "RESULTS_PATH = \"fedavg_grid_results.json\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Loading CIFAR-100 ---\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- FL split: IID sharding + train/val local ---\n",
        "client_data = iid_shard_train_val(full_train, K=K, val_split=0.2, seed=42)\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    train_idxs = client_data[i]['train']\n",
        "    client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "    clients.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "val_indices = np.concatenate([client_data[i]['val'] for i in range(K)])\n",
        "val_set = TransformedSubset(Subset(full_train, val_indices), val_transform)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False)\n",
        "\n",
        "# --- Global model ViT-S/16 DINO CIFAR-100 ---\n",
        "def make_model():\n",
        "    return DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def eval_fn(model):\n",
        "    return evaluate(model, val_loader, device)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']})\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def plot_history(acc_history, loss_history, eval_every):\n",
        "    rounds = np.arange(0, len(acc_history))*eval_every + eval_every\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(rounds, acc_history, label='Val Acc')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Val Accuracy')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(rounds, loss_history, label='Val Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Val Loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def fit_with_all_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=None, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    if resume and checkpoint_path:\n",
        "        start_round, acc_history, loss_history = load_checkpoint(self.global_model, checkpoint_path)\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            val_acc, val_loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Val Acc={val_acc:.3f} | Val Loss={val_loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"val_acc\": val_acc, \"val_loss\": val_loss})\n",
        "            acc_history.append(val_acc)\n",
        "            loss_history.append(val_loss)\n",
        "            if checkpoint_path and (rnd % 5 == 0 or rnd == n_rounds):\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "        if rnd % 5 == 0 or rnd == n_rounds:\n",
        "            plot_history(acc_history, loss_history, eval_every)\n",
        "    # Log best hyperparams for this run\n",
        "    best_acc = max(acc_history) if acc_history else 0\n",
        "    result = dict(wandb.config)\n",
        "    result[\"best_val_acc\"] = best_acc\n",
        "    with open(RESULTS_PATH, \"a\") as f:\n",
        "        f.write(json.dumps(result) + \"\\n\")\n",
        "\n",
        "# --- GRID SEARCH RUNS ---\n",
        "run_idx = 0\n",
        "for lr in lr_list:\n",
        "    for momentum in momentum_list:\n",
        "        run_idx += 1\n",
        "        print(f\"\\n=== NEW HP RUN {run_idx}: lr={lr}, momentum={momentum} ===\\n\")\n",
        "        global_model = make_model()\n",
        "\n",
        "        run_name = f\"fedavg_iid_grid_lr{lr}_mom{momentum}_J{J}_nrounds{n_rounds}_bs{batch_size}\"\n",
        "\n",
        "        wandb.init(\n",
        "            project=\"fl-fedavg\",\n",
        "            name=run_name,\n",
        "            config={\n",
        "                \"model\": \"DINO ViT-S/16\",\n",
        "                \"K\": K,\n",
        "                \"C\": C,\n",
        "                \"J\": J,\n",
        "                \"n_rounds\": n_rounds,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"lr\": lr,\n",
        "                \"momentum\": momentum,\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"sharding\": \"iid\",\n",
        "                \"Nc\": None,\n",
        "                \"use_sparse\": False\n",
        "            }\n",
        "        )\n",
        "\n",
        "        trainer = FederatedTrainer(\n",
        "            clients=clients,\n",
        "            global_model=global_model,\n",
        "            device=device,\n",
        "            client_fraction=C,\n",
        "            local_epochs=J,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            weight_decay=weight_decay,\n",
        "            scheduler_fn=make_scheduler,\n",
        "            use_sparse=False\n",
        "        )\n",
        "\n",
        "        trainer.fit = types.MethodType(fit_with_all_logs, trainer)\n",
        "        trainer.fit(\n",
        "            n_rounds,\n",
        "            eval_fn=eval_fn,\n",
        "            eval_every=2,\n",
        "            checkpoint_path=None,\n",
        "            resume=False\n",
        "        )\n",
        "        wandb.finish()\n"
      ],
      "metadata": {
        "id": "qYNdf6qqooT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FedAvg Non IID"
      ],
      "metadata": {
        "id": "EGDHSVO9uNvR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This script runs a full non-i.i.d. grid search over multiple combinations of local epochs (Jc), communication rounds, and class partitions per client (Nc). For each configuration, a new non-i.i.d. sharding is generated, a new global model is initialized, and a separate training run is launched using the FedAvg algorithm on the CIFAR-100 dataset with a DINO ViT-S/16 model. Training is logged with Weights & Biases, checkpoints are saved for resuming or analysis, and performance metrics (accuracy and loss) are plotted and stored for each experiment."
      ],
      "metadata": {
        "id": "skoOi12SjSXC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federated Grid Search under Non-IID Conditions\n",
        "\n",
        "This cell runs a comprehensive set of Federated Learning experiments using the FedAvg algorithm under non-i.i.d. conditions on the CIFAR-100 dataset. It systematically explores the impact of different degrees of data heterogeneity and local update intensity.\n",
        "\n",
        "### Dataset Setup\n",
        "- CIFAR-100 is partitioned across 100 clients using **non-i.i.d. label-skew** splitting, controlled by the `Nc` parameter (number of classes per client).\n",
        "- Each client receives a personalized dataset along with appropriate train transformations.\n",
        "- A global validation set is built from the union of all clients' validation splits.\n",
        "\n",
        "### Fixed & Tuned Hyperparameters\n",
        "- The best learning rate (`lr=0.001`) and momentum (`momentum=0.8`) values found in previous i.i.d. experiments are reused.\n",
        "- Three values for local epochs (`Jc = 4, 8, 16`) are paired with decreasing numbers of communication rounds (`n_rounds = 50, 25, 12`) to maintain a constant local workload.\n",
        "- The number of classes per client (`Nc`) is varied over `[1, 5, 10, 50]` to simulate increasing degrees of heterogeneity.\n",
        "\n",
        "### Training Workflow\n",
        "- For each `(Jc, Nc)` combination, a new `FederatedTrainer` and `DinoViT_CIFAR100` model are instantiated.\n",
        "- Training proceeds for `n_rounds` with client sampling fraction `C = 0.1`, using FedAvg for aggregation.\n",
        "- Validation performance is evaluated every 2 rounds and logged to Weights & Biases.\n",
        "- Checkpoints are saved periodically in a dedicated directory (`fedavg_non_iid_ckpts`), and the best results are stored in a results file (`fedavg_non_iid_results.json`).\n",
        "\n",
        "This experimental protocol allows fine-grained analysis of how local update intensity and data heterogeneity affect convergence and generalization in federated settings.\n"
      ],
      "metadata": {
        "id": "Ug3v0myXs4pE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import wandb\n",
        "import types\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "K = 100\n",
        "C = 0.1\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "\n",
        "Jc_list = [4, 8, 16]\n",
        "n_rounds_list = [50, 25, 12]\n",
        "Nc_list = [1, 5, 10, 50]\n",
        "\n",
        "RESULTS_PATH = \"fedavg_non_iid_results.json\"\n",
        "CKPT_DIR = \"fedavg_non_iid_ckpts\"\n",
        "os.makedirs(CKPT_DIR, exist_ok=True)\n",
        "\n",
        "# --- Transforms ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "def make_scheduler(optimizer, J_value):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J_value)\n",
        "\n",
        "# ==== GRIDSEARCH NON-IID FL-READY ====\n",
        "run_idx = 0\n",
        "\n",
        "for Jc, n_rounds in zip(Jc_list, n_rounds_list):\n",
        "    for Nc in Nc_list:\n",
        "        run_idx += 1\n",
        "\n",
        "        non_iid_split = non_iid_shard_train_val(full_train, K=K, Nc=Nc, val_split=0.2, seed=42)\n",
        "        clients = []\n",
        "        for i in range(K):\n",
        "            train_idxs = non_iid_split[i]['train']\n",
        "            client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "            clients.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "        val_indices = np.concatenate([non_iid_split[i]['val'] for i in range(K)])\n",
        "        val_set = TransformedSubset(Subset(full_train, val_indices), val_transform)\n",
        "        val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False)\n",
        "\n",
        "        global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "        ckpt_name = f\"fedavg_non_iid_checkpoint_Nc{Nc}_J{Jc}_nrounds{n_rounds}.pt\"\n",
        "        checkpoint_path = os.path.join(CKPT_DIR, ckpt_name)\n",
        "\n",
        "        run_name = f\"fedavg_non_iid_J{Jc}_nrounds{n_rounds}_Nc{Nc}_lr{lr}_mom{momentum}\"\n",
        "        wandb.init(\n",
        "            project=\"fl-fedavg\",\n",
        "            name=run_name,\n",
        "            config={\n",
        "                \"model\": \"DINO ViT-S/16\",\n",
        "                \"K\": K,\n",
        "                \"C\": C,\n",
        "                \"J\": Jc,\n",
        "                \"n_rounds\": n_rounds,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"lr\": lr,\n",
        "                \"momentum\": momentum,\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"sharding\": \"non-iid\",\n",
        "                \"Nc\": Nc,\n",
        "                \"use_sparse\": False\n",
        "            }\n",
        "        )\n",
        "\n",
        "        trainer = FederatedTrainer(\n",
        "            clients=clients,\n",
        "            global_model=global_model,\n",
        "            device=device,\n",
        "            client_fraction=C,\n",
        "            local_epochs=Jc,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            weight_decay=weight_decay,\n",
        "            scheduler_fn=lambda opt: make_scheduler(opt, Jc),\n",
        "            use_sparse=False\n",
        "        )\n",
        "\n",
        "        def eval_fn(model):\n",
        "            return evaluate(model, val_loader, device)\n",
        "\n",
        "        trainer.fit = types.MethodType(fit_with_all_logs, trainer)\n",
        "        print(f\"\\n=== RUN {run_idx}: J={Jc}, n_rounds={n_rounds}, Nc={Nc} ===\\n\")\n",
        "        trainer.fit(n_rounds, eval_fn=eval_fn, eval_every=2, checkpoint_path=checkpoint_path, resume=True)\n",
        "        wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "id": "nH0MwFRRieJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Editing FedAvg IID, Research of best hyperparameters"
      ],
      "metadata": {
        "id": "iM642rTSuiFy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is IID model Editing code.We are looking for the best hyperparams on 50 rounds. This code runs a grid search over various sparsity ratios and calibration rounds to evaluate the impact of model editing using sparse fine-tuning in a federated learning setting. Each configuration corresponds to a separate run, where a DINO ViT-S/16 model is trained with i.i.d. sharding on CIFAR-100 using the FedAvg algorithm. Although this code was executed on a different Google Colab instance, all training curves and metrics remain accessible via the associated Weights & Biases (wandb) project, enabling transparent and centralized monitoring across experiments."
      ],
      "metadata": {
        "id": "PjJFWXTfjy5y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federated Sparse Fine-Tuning via Model Editing (IID Setup)\n",
        "\n",
        "This cell launches a Federated Learning experiment using **model editing via sparse fine-tuning** under i.i.d. conditions, based on a DINO-pretrained ViT-S/16 model and the CIFAR-100 dataset.\n",
        "\n",
        "### Objective\n",
        "The goal is to assess the impact of sparse fine-tuning on global model performance by updating only a subset of parameters per communication round. This is achieved through binary gradient masks calibrated via approximated Fisher Information.\n",
        "\n",
        "### Experimental Settings\n",
        "- **Fixed Parameters**:\n",
        "  - `K=100` clients, with a sampling fraction `C=0.1` per round.\n",
        "  - Local training: `J=4` epochs per round.\n",
        "  - **Learning rate (`lr=0.001`) and momentum (`momentum=0.8`) are reused from the best configuration found in the standard IID FedAvg grid search**.\n",
        "  - Additional fixed values: `weight_decay=5e-4`, `batch_size=128`.\n",
        "\n",
        "- **Grid Search Parameters**:\n",
        "  - `sparsity_ratio ∈ {0.85, 0.90, 0.95}`: proportion of weights masked out (not updated).\n",
        "  - `num_calib_rounds ∈ {1, 3, 5}`: number of rounds used to progressively refine the gradient mask.\n",
        "  - `sparse_ft_epochs = 1`: number of epochs used during the sparse fine-tuning phase on each client.\n",
        "\n",
        "### Model Editing Logic\n",
        "For each client:\n",
        "- A **gradient mask** is calibrated using approximated Fisher scores computed over local batches.\n",
        "- Only parameters with high sensitivity (mask = 1) are updated during sparse fine-tuning.\n",
        "- Fine-tuning is conducted via SGD, with masked gradients zeroed out via `SparseSGDM`.\n",
        "\n",
        "### Logging & Checkpointing\n",
        "- Performance metrics are logged to Weights & Biases (wandb) every few rounds.\n",
        "- Model checkpoints are saved periodically and at the final round.\n",
        "- The best validation accuracy and corresponding hyperparameters are saved to a JSON file for post-analysis.\n",
        "\n",
        "This pipeline enables a rigorous evaluation of model editing techniques through sparsity-aware federated training, offering insight into the efficiency and effectiveness of selective parameter updates.\n"
      ],
      "metadata": {
        "id": "BXyo6yfwtXsX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import wandb\n",
        "import types\n",
        "import os\n",
        "import json\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Hyperparams\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 50\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "\n",
        "# Param grid\n",
        "sparsity_ratios = [0.85, 0.90, 0.95]\n",
        "num_calib_rounds_list = [1, 3, 5]\n",
        "sparse_ft_epochs = 1\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Data (no split/transform here) ---\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "iid_split = iid_shard_train_val(full_train, K=K, val_split=0.2, seed=42)\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "val_indices = np.concatenate([iid_split[i]['val'] for i in range(K)])\n",
        "val_set = TransformedSubset(Subset(full_train, val_indices), val_transform)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def eval_fn_edit(model):\n",
        "    return evaluate(model, val_loader, device)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']}) from {path}\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def save_best_hyperparams(acc_history, config, path):\n",
        "    best_acc = max(acc_history) if acc_history else 0.0\n",
        "    run_data = {\"best_val_acc\": best_acc}\n",
        "    run_data.update(config)\n",
        "    with open(path, \"a\") as f:\n",
        "        f.write(json.dumps(run_data) + \"\\n\")\n",
        "\n",
        "def fit_with_wandb_and_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=None, best_json_path=None, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    if resume and checkpoint_path:\n",
        "        start_round, acc_history, loss_history = load_checkpoint(self.global_model, checkpoint_path)\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} (SPARSE-EDITING) ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            acc, loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Eval: Acc={acc:.3f} | Loss={loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"val_acc\": acc, \"val_loss\": loss})\n",
        "            acc_history.append(acc)\n",
        "            loss_history.append(loss)\n",
        "            if checkpoint_path and (rnd % 5 == 0 or rnd == n_rounds):\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "    if best_json_path:\n",
        "        save_best_hyperparams(acc_history, wandb.config, best_json_path)\n",
        "\n",
        "\n",
        "# === GRIDSEARCH MODEL EDITING FL-READY ===\n",
        "run_idx = 0\n",
        "for sparsity_ratio in sparsity_ratios:\n",
        "    for num_calib_rounds in num_calib_rounds_list:\n",
        "        run_idx += 1\n",
        "        print(f\"\\n=== MODEL EDITING RUN {run_idx}/9 ===\\n\")\n",
        "        clients_edit = []\n",
        "        for i in range(K):\n",
        "            train_idxs = iid_split[i]['train']\n",
        "            client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "            clients_edit.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "        global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "        run_name = (f\"model_editing_iid_nrounds{n_rounds}_lr{lr}_\"\n",
        "                    f\"sp{int(sparsity_ratio*100)}_calib{num_calib_rounds}_ftep{sparse_ft_epochs}\")\n",
        "        checkpoint_path = f\"{run_name}.pt\"\n",
        "        best_json_path = f\"{run_name}.json\"\n",
        "\n",
        "        wandb.init(\n",
        "            project=\"fl-fedavg\",\n",
        "            name=run_name,\n",
        "            config={\n",
        "                \"model\": \"DINO ViT-S/16\",\n",
        "                \"K\": K,\n",
        "                \"C\": C,\n",
        "                \"J\": J,\n",
        "                \"n_rounds\": n_rounds,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"lr\": lr,\n",
        "                \"momentum\": momentum,\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"sharding\": \"iid\",\n",
        "                \"use_sparse\": True,\n",
        "                \"sparsity_ratio\": sparsity_ratio,\n",
        "                \"num_calib_rounds\": num_calib_rounds,\n",
        "                \"sparse_ft_epochs\": sparse_ft_epochs\n",
        "            }\n",
        "        )\n",
        "\n",
        "        trainer_edit = FederatedTrainer(\n",
        "            clients=clients_edit,\n",
        "            global_model=global_model,\n",
        "            device=device,\n",
        "            client_fraction=C,\n",
        "            local_epochs=J,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            weight_decay=weight_decay,\n",
        "            scheduler_fn=make_scheduler,\n",
        "            use_sparse=True,\n",
        "            sparsity_ratio=sparsity_ratio,\n",
        "            num_calib_rounds=num_calib_rounds,\n",
        "            sparse_ft_epochs=sparse_ft_epochs\n",
        "        )\n",
        "        trainer_edit.fit = types.MethodType(fit_with_wandb_and_logs, trainer_edit)\n",
        "        trainer_edit.fit(\n",
        "            n_rounds,\n",
        "            eval_fn=eval_fn_edit,\n",
        "            eval_every=2,\n",
        "            checkpoint_path=checkpoint_path,\n",
        "            best_json_path=best_json_path,\n",
        "            resume=True\n",
        "        )\n",
        "        wandb.finish()\n",
        "\n"
      ],
      "metadata": {
        "id": "06Y4ofl7Rfst"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Editing FedAvg non IID, Research of best hyperparameters"
      ],
      "metadata": {
        "id": "Nnr8vL_dU2je"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Federated Sparse Fine-Tuning via Model Editing (Non-IID Setup)\n",
        "\n",
        "This experiment replicates the model editing strategy with **sparse fine-tuning**, this time under **non-i.i.d. conditions** where each client receives data from only a limited number of classes (`Nc = 50`). The CIFAR-100 dataset is distributed in a label-skewed fashion to better simulate realistic statistical heterogeneity.\n",
        "\n",
        "### Objective\n",
        "To evaluate how sparse fine-tuning performs in non-i.i.d. settings when clients calibrate personalized update masks based on their own local data distributions.\n",
        "\n",
        "### Experimental Settings\n",
        "- **Fixed Parameters**:\n",
        "  - `K = 100` clients, with a participation fraction `C = 0.1` per round.\n",
        "  - Each round consists of `J = 4` local epochs.\n",
        "  - **Learning rate (`lr = 0.001`) and momentum (`momentum = 0.8`) are inherited from the best IID baseline**.\n",
        "  - `Nc = 50`: each client receives samples from 50 unique classes.\n",
        "  - `batch_size = 128`, `weight_decay = 5e-4`.\n",
        "\n",
        "- **Grid Search Parameters**:\n",
        "  - `sparsity_ratio ∈ {0.85, 0.90, 0.95}`: controls how many weights are frozen.\n",
        "  - `num_calib_rounds ∈ {1, 3, 5}`: number of rounds used to progressively calibrate the sparse mask.\n",
        "  - `sparse_ft_epochs = 1`: number of fine-tuning epochs on each client.\n",
        "\n",
        "### Model Editing Logic\n",
        "Each client:\n",
        "- Computes an importance mask using approximated Fisher Information.\n",
        "- Applies the mask to freeze less sensitive parameters.\n",
        "- Fine-tunes only the most important subset using SGD.\n",
        "\n",
        "A central validation set is created by aggregating all local validation sets. Performance is evaluated every two rounds.\n",
        "\n",
        "### Logging and Saving\n",
        "- Model metrics are logged to **Weights & Biases (wandb)**.\n",
        "- Checkpoints are saved every 5 rounds or at the end of training.\n",
        "- Each run's best validation accuracy and associated hyperparameters are stored in a JSON file for analysis.\n",
        "\n",
        "This pipeline allows assessing the robustness and efficiency of sparse model editing techniques in the presence of non-i.i.d. data distributions.\n"
      ],
      "metadata": {
        "id": "fDGin9bQt0-L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import wandb\n",
        "import types\n",
        "import os\n",
        "import json\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "J = 4\n",
        "C = 0.1\n",
        "Nc = 50\n",
        "n_rounds = 50\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "sparsity_ratios = [0.85, 0.90, 0.95]\n",
        "num_calib_rounds_list = [1, 3, 5]\n",
        "sparse_ft_epochs = 1\n",
        "K = 100\n",
        "\n",
        "\n",
        "seed = 42\n",
        "val_split = 0.2\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "non_iid_split = non_iid_shard_train_val(full_train, K=K, Nc=Nc, val_split=val_split, seed=seed)\n",
        "\n",
        "class TransformedSubset(torch.utils.data.Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Global Validation\n",
        "val_indices = np.concatenate([non_iid_split[i]['val'] for i in range(K)])\n",
        "val_set = TransformedSubset(torch.utils.data.Subset(full_train, val_indices), val_transform)\n",
        "val_loader = torch.utils.data.DataLoader(val_set, batch_size=128, shuffle=False)\n",
        "\n",
        "\n",
        "def eval_fn_edit(model):\n",
        "    return evaluate(model, val_loader, device)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']}) from {path}\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def save_best_hyperparams(acc_history, config, path):\n",
        "    best_acc = max(acc_history) if acc_history else 0.0\n",
        "    run_data = {\"best_val_acc\": best_acc}\n",
        "    run_data.update(config)\n",
        "    with open(path, \"a\") as f:\n",
        "        f.write(json.dumps(run_data) + \"\\n\")\n",
        "\n",
        "def fit_with_wandb_and_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=None, best_json_path=None, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    if resume and checkpoint_path:\n",
        "        start_round, acc_history, loss_history = load_checkpoint(self.global_model, checkpoint_path)\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} (SPARSE-EDITING) ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            acc, loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Eval: Acc={acc:.3f} | Loss={loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"val_acc\": acc, \"val_loss\": loss})\n",
        "            acc_history.append(acc)\n",
        "            loss_history.append(loss)\n",
        "            if checkpoint_path and (rnd % 5 == 0 or rnd == n_rounds):\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "    if best_json_path:\n",
        "        save_best_hyperparams(acc_history, wandb.config, best_json_path)\n",
        "\n",
        "# --- GRIDSEARCH MODEL EDITING NON-IID ---\n",
        "run_idx = 0\n",
        "for sparsity_ratio in sparsity_ratios:\n",
        "    for num_calib_rounds in num_calib_rounds_list:\n",
        "        run_idx += 1\n",
        "        print(f\"\\n=== MODEL EDITING NON-IID RUN {run_idx}/9 ===\\n\")\n",
        "        # Clients FL-ready (non-iid)\n",
        "        clients_edit = []\n",
        "        for i in range(K):\n",
        "            train_idxs = non_iid_split[i]['train']\n",
        "            client_train_dataset = TransformedSubset(torch.utils.data.Subset(full_train, train_idxs), train_transform)\n",
        "            clients_edit.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "        global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "        run_name = (f\"model_editing_non_iid_J{J}_nrounds{n_rounds}_Nc{Nc}_lr{lr}_\"\n",
        "                    f\"sp{int(sparsity_ratio*100)}_calib{num_calib_rounds}_ftep{sparse_ft_epochs}\")\n",
        "        checkpoint_path = f\"{run_name}.pt\"\n",
        "        best_json_path = f\"{run_name}.json\"\n",
        "\n",
        "        wandb.init(\n",
        "            project=\"fl-fedavg\",\n",
        "            name=run_name,\n",
        "            config={\n",
        "                \"model\": \"DINO ViT-S/16\",\n",
        "                \"K\": K,\n",
        "                \"C\": C,\n",
        "                \"J\": J,\n",
        "                \"n_rounds\": n_rounds,\n",
        "                \"Nc\": Nc,\n",
        "                \"batch_size\": batch_size,\n",
        "                \"lr\": lr,\n",
        "                \"momentum\": momentum,\n",
        "                \"weight_decay\": weight_decay,\n",
        "                \"sharding\": \"non-iid\",\n",
        "                \"use_sparse\": True,\n",
        "                \"sparsity_ratio\": sparsity_ratio,\n",
        "                \"num_calib_rounds\": num_calib_rounds,\n",
        "                \"sparse_ft_epochs\": sparse_ft_epochs\n",
        "            }\n",
        "        )\n",
        "\n",
        "        trainer_edit = FederatedTrainer(\n",
        "            clients=clients_edit,\n",
        "            global_model=global_model,\n",
        "            device=device,\n",
        "            client_fraction=C,\n",
        "            local_epochs=J,\n",
        "            batch_size=batch_size,\n",
        "            lr=lr,\n",
        "            momentum=momentum,\n",
        "            weight_decay=weight_decay,\n",
        "            scheduler_fn=make_scheduler,\n",
        "            use_sparse=True,\n",
        "            sparsity_ratio=sparsity_ratio,\n",
        "            num_calib_rounds=num_calib_rounds,\n",
        "            sparse_ft_epochs=sparse_ft_epochs\n",
        "        )\n",
        "        trainer_edit.fit = types.MethodType(fit_with_wandb_and_logs, trainer_edit)\n",
        "        trainer_edit.fit(\n",
        "            n_rounds,\n",
        "            eval_fn=eval_fn_edit,\n",
        "            eval_every=2,\n",
        "            checkpoint_path=checkpoint_path,\n",
        "            best_json_path=best_json_path,\n",
        "            resume=True\n",
        "        )\n",
        "        wandb.finish()\n"
      ],
      "metadata": {
        "id": "OI4GywkeRirX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loading of same dataset with split train / test\n"
      ],
      "metadata": {
        "id": "d7QzMpEdYZ4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Dataset Freezing for Evaluation and Reuse\n",
        "\n",
        "This cell prepares and freezes the dataset partitions for future experimental runs under both i.i.d. and non-i.i.d. settings **without any validation split** (`val_split = 0`), making it suitable for training or inference-only workflows.\n",
        "\n",
        "### CIFAR-100 Dataset\n",
        "- The full CIFAR-100 training and test sets are loaded once without transformations.\n",
        "- Validation sets are omitted for this configuration, i.e., all available data is allocated to training per client.\n",
        "\n",
        "### Sharding Strategies\n",
        "- **IID Split**: Clients receive a uniformly distributed subset of all classes, ensuring a balanced and representative dataset.\n",
        "- **Non-IID Split**: Clients receive examples from `Nc = 50` distinct classes, simulating partial class visibility and label skew typical in federated scenarios.\n",
        "\n",
        "### Reusability\n",
        "- The resulting splits (`iid_split` and `non_iid_split`) are stored in memory as dictionaries mapping each client ID to its training indices.\n",
        "- These static splits can now be reused **consistently across multiple runs and model variants**, ensuring fair and reproducible comparisons.\n",
        "\n",
        "This setup is especially useful for locked-in evaluation stages, test-time performance comparisons, or fixed-data ablation studies.\n"
      ],
      "metadata": {
        "id": "KVnUX6C4uDqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "\n",
        "# Hyperparams\n",
        "K = 100\n",
        "val_split = 0\n",
        "seed = 42\n",
        "Nc = 50  # <-- choisis ici la valeur de Nc que tu veux pour le non-iid\n",
        "\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "iid_split = iid_shard_train_val(full_train, K=K, val_split=val_split, seed=seed)\n",
        "\n",
        "non_iid_split = non_iid_shard_train_val(full_train, K=K, Nc=Nc, val_split=val_split, seed=seed)\n"
      ],
      "metadata": {
        "id": "l_hjJHd9YZSB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Accuracy FedAvg IID"
      ],
      "metadata": {
        "id": "fCLD_upD9JwV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Evaluation of the FedAvg Baseline on Test Set Only\n",
        "\n",
        "This cell performs the **final evaluation** of the global model trained using FedAvg in **IID configuration**. The focus is solely on the **test set**, in order to assess the **generalization performance** of the model on unseen data.\n",
        "\n",
        "### Key Features of This Evaluation:\n",
        "- **Client Data**: Clients were initially sharded in an **IID** manner with `val_split = 0`, meaning no local validation was used to preserve the **full training capacity**.\n",
        "- **Model**: A `DINO ViT-S/16` backbone (pretrained), adapted for CIFAR-100 with a linear classification head.\n",
        "- **Test Set**: The CIFAR-100 test set is fully normalized and used without any stochastic data augmentation.\n",
        "- **Tracking**: Every evaluable `round` logs `test accuracy` and `test loss` to **WandB**, while dynamically generating **performance plots**.\n",
        "\n",
        "### Training Setup\n",
        "The hyperparameters (batch size, learning rate, momentum) are those obtained from the **best-performing settings** during previous classical IID FedAvg experiments.\n",
        "\n",
        "This cell is intended to be used **after training**, to produce a **standardized, reproducible, and trackable** final evaluation of the global model.\n"
      ],
      "metadata": {
        "id": "LTMMtfj4uUJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== FL READY: DATA, CLIENTS, WANDB, FEDAVG BASELINE ======\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import wandb\n",
        "import types\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- FL params ---\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 50\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "\n",
        "CHECKPOINT_PATH = \"fedavg_checkpoint.pt\"\n",
        "BEST_HYPERPARAMS_PATH = \"best_run.json\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Loading CIFAR-100  ---\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- FL split: IID sharding + train local ---\n",
        "client_data = iid_split\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    train_idxs = client_data[i]['train']\n",
        "    client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "    clients.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    TransformedSubset(test_set, val_transform), batch_size=128, shuffle=False\n",
        ")\n",
        "\n",
        "# --- Global model ViT-S/16 DINO CIFAR-100 ---\n",
        "global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "# --- Eval function (TEST ONLY) ---\n",
        "def eval_fn_test(model):\n",
        "    return evaluate(model, test_loader, device)\n",
        "\n",
        "# --- Logging, checkpoint, plotting utils ---\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path=CHECKPOINT_PATH):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path=CHECKPOINT_PATH):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']})\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def plot_history(acc_history, loss_history, eval_every):\n",
        "    rounds = np.arange(0, len(acc_history))*eval_every + eval_every\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(rounds, acc_history, label='Test Acc')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(rounds, loss_history, label='Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def fit_with_all_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=CHECKPOINT_PATH, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    if resume:\n",
        "        start_round, acc_history, loss_history = load_checkpoint(self.global_model, checkpoint_path)\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            test_acc, test_loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Test Acc={test_acc:.3f} | Test Loss={test_loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"test_acc\": test_acc, \"test_loss\": test_loss})\n",
        "            acc_history.append(test_acc)\n",
        "            loss_history.append(test_loss)\n",
        "            if rnd % 5 == 0 or rnd == n_rounds:\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "        if rnd % 5 == 0 or rnd == n_rounds:\n",
        "            plot_history(acc_history, loss_history, eval_every)\n",
        "\n",
        "\n",
        "# --- WANDB init ---\n",
        "wandb.init(\n",
        "    project=\"fl-fedavg\",\n",
        "    name=f\"fedavg_iid_baseline_test_acc_J{J}_nrounds{n_rounds}_lr{lr}\",\n",
        "    config={\n",
        "        \"model\": \"DINO ViT-S/16\",\n",
        "        \"K\": K,\n",
        "        \"C\": C,\n",
        "        \"J\": J,\n",
        "        \"n_rounds\": n_rounds,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr,\n",
        "        \"momentum\": momentum,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"sharding\": \"iid\",\n",
        "        \"Nc\": None,\n",
        "        \"use_sparse\": False\n",
        "    }\n",
        ")\n",
        "\n",
        "# --- Federated trainer FL-ready ---\n",
        "trainer = FederatedTrainer(\n",
        "    clients=clients,\n",
        "    global_model=global_model,\n",
        "    device=device,\n",
        "    client_fraction=C,\n",
        "    local_epochs=J,\n",
        "    batch_size=batch_size,\n",
        "    lr=lr,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    scheduler_fn=make_scheduler,\n",
        "    use_sparse=False\n",
        ")\n",
        "\n",
        "# --- Patch et run ---\n",
        "trainer.fit = types.MethodType(fit_with_all_logs, trainer)\n",
        "trainer.fit(n_rounds, eval_fn=eval_fn_test, eval_every=2, checkpoint_path=CHECKPOINT_PATH, resume=True)\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "kut1DyOrOQvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## FedAvg Non IID Test Accuracy"
      ],
      "metadata": {
        "id": "xNydkdvo-W0p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Evaluation of the FedAvg Baseline on Non-IID Test Set\n",
        "\n",
        "This experiment performs the **final evaluation** of a global model trained with the **FedAvg algorithm** under a **non-IID client distribution**. The primary goal is to assess how well the model generalizes to unseen data when local training data is significantly biased across clients.\n",
        "\n",
        "### Key Points:\n",
        "- **Non-IID Setup**: Each of the `K=100` clients is assigned data from a fixed number of `Nc` distinct classes. This configuration simulates real-world heterogeneity in federated learning systems.\n",
        "- **Test-Only Evaluation**: The model is evaluated on the **official CIFAR-100 test set**, fully normalized and independent of all clients.\n",
        "- **Model**: Uses a DINO ViT-S/16 backbone pretrained on ImageNet, with a lightweight classifier head adapted for 100-way classification.\n",
        "- **Metrics Logged**: Test accuracy and loss are recorded every 2 rounds to **Weights & Biases**, alongside automatic plotting and checkpoint saving for reproducibility.\n",
        "\n"
      ],
      "metadata": {
        "id": "ZK0UnP95u1es"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== FL READY: DATA, CLIENTS, WANDB, FEDAVG NON-IID BASELINE ======\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import wandb\n",
        "import types\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- FL params ---\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 50\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "Nc = Nc\n",
        "\n",
        "CHECKPOINT_PATH = \"fedavg_non_iid_checkpoint.pt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Loading CIFAR-100  ---\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- FL split: NON-IID sharding + train/val local ---\n",
        "client_data = non_iid_split\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    train_idxs = client_data[i]['train']\n",
        "    client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "    clients.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    TransformedSubset(test_set, val_transform), batch_size=128, shuffle=False\n",
        ")\n",
        "\n",
        "# --- Global model ViT-S/16 DINO CIFAR-100 ---\n",
        "global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def eval_fn_test(model):\n",
        "    return evaluate(model, test_loader, device)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path=CHECKPOINT_PATH):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path=CHECKPOINT_PATH):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']})\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def plot_history(acc_history, loss_history, eval_every):\n",
        "    rounds = np.arange(0, len(acc_history))*eval_every + eval_every\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(rounds, acc_history, label='Test Acc')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(rounds, loss_history, label='Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def fit_with_all_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=CHECKPOINT_PATH, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    if resume:\n",
        "        start_round, acc_history, loss_history = load_checkpoint(self.global_model, checkpoint_path)\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            test_acc, test_loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Test Acc={test_acc:.3f} | Test Loss={test_loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"test_acc\": test_acc, \"test_loss\": test_loss})\n",
        "            acc_history.append(test_acc)\n",
        "            loss_history.append(test_loss)\n",
        "            if rnd % 5 == 0 or rnd == n_rounds:\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "        if rnd % 5 == 0 or rnd == n_rounds:\n",
        "            plot_history(acc_history, loss_history, eval_every)\n",
        "\n",
        "# --- WANDB init ---\n",
        "wandb.init(\n",
        "    project=\"fl-fedavg\",\n",
        "    name=f\"fedavg_non_iid_baseline_test_acc_J{J}_nrounds{n_rounds}_lr{lr}_Nc{Nc}\",\n",
        "    config={\n",
        "        \"model\": \"DINO ViT-S/16\",\n",
        "        \"K\": K,\n",
        "        \"C\": C,\n",
        "        \"J\": J,\n",
        "        \"n_rounds\": n_rounds,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr,\n",
        "        \"momentum\": momentum,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"sharding\": \"non-iid\",\n",
        "        \"Nc\": Nc,\n",
        "        \"use_sparse\": False\n",
        "    }\n",
        ")\n",
        "\n",
        "# --- Federated trainer FL-ready ---\n",
        "trainer = FederatedTrainer(\n",
        "    clients=clients,\n",
        "    global_model=global_model,\n",
        "    device=device,\n",
        "    client_fraction=C,\n",
        "    local_epochs=J,\n",
        "    batch_size=batch_size,\n",
        "    lr=lr,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    scheduler_fn=make_scheduler,\n",
        "    use_sparse=False\n",
        ")\n",
        "\n",
        "trainer.fit = types.MethodType(fit_with_all_logs, trainer)\n",
        "trainer.fit(n_rounds, eval_fn=eval_fn_test, eval_every=2, checkpoint_path=CHECKPOINT_PATH, resume=True)\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "JqtNbRB9-ZRl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Accuracy FedAvg IID Model Editing"
      ],
      "metadata": {
        "id": "o0nv-P5q-8I5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Evaluation of Model Editing under IID Setup (Test Accuracy)\n",
        "\n",
        "This experiment evaluates the final performance of the **Model Editing technique** under an **IID client distribution**. The model is trained with **sparse fine-tuning**, using a calibrated gradient mask and limited parameter updates per round.\n",
        "\n",
        "### Key Features:\n",
        "- **IID Distribution**: All clients are assigned random, balanced subsets of the dataset (uniform over all classes).\n",
        "- **Model Editing Strategy**: A fixed sparsity mask (ratio = 85%) is calibrated over `5` rounds and used to restrict updates to a subset of parameters. Fine-tuning is done with `1` epoch locally.\n",
        "- **Global Evaluation**: Performance is measured on the CIFAR-100 **official test set**, ensuring consistency with the FedAvg baseline.\n",
        "- **Model**: A ViT-S/16 pretrained with DINO is used as the backbone, with a new classifier head.\n",
        "- **Metrics**: Test accuracy and loss are logged every 2 rounds to **Weights & Biases** with visual plots and periodic checkpoint saving.\n",
        "\n"
      ],
      "metadata": {
        "id": "L5DRKXJpvQrj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== FL READY: DATA, CLIENTS, WANDB, MODEL EDITING IID TEST ACC ======\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import wandb\n",
        "import types\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- FL params ---\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 50\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "\n",
        "# --- Model Editing HP choosen ---\n",
        "sparsity_ratio = 0.85\n",
        "num_calib_rounds = 5\n",
        "sparse_ft_epochs = 1\n",
        "\n",
        "CHECKPOINT_PATH = \"model_editing_iid_checkpoint.pt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- FL split: IID sharding + train local ---\n",
        "client_data = iid_split\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    train_idxs = client_data[i]['train']\n",
        "    client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "    clients.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    TransformedSubset(test_set, val_transform), batch_size=128, shuffle=False\n",
        ")\n",
        "\n",
        "# --- Global model ViT-S/16 DINO CIFAR-100 ---\n",
        "global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def eval_fn_test(model):\n",
        "    return evaluate(model, test_loader, device)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path=CHECKPOINT_PATH):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path=CHECKPOINT_PATH):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']})\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def plot_history(acc_history, loss_history, eval_every):\n",
        "    rounds = np.arange(0, len(acc_history))*eval_every + eval_every\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(rounds, acc_history, label='Test Acc')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(rounds, loss_history, label='Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def fit_with_all_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=CHECKPOINT_PATH, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    if resume:\n",
        "        start_round, acc_history, loss_history = load_checkpoint(self.global_model, checkpoint_path)\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} (MODEL EDITING) ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            test_acc, test_loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Test Acc={test_acc:.3f} | Test Loss={test_loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"test_acc\": test_acc, \"test_loss\": test_loss})\n",
        "            acc_history.append(test_acc)\n",
        "            loss_history.append(test_loss)\n",
        "            if rnd % 5 == 0 or rnd == n_rounds:\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "        if rnd % 5 == 0 or rnd == n_rounds:\n",
        "            plot_history(acc_history, loss_history, eval_every)\n",
        "\n",
        "# --- WANDB init ---\n",
        "wandb.init(\n",
        "    project=\"fl-fedavg\",\n",
        "    name=f\"model_editing_iid_test_acc_J{J}_nrounds{n_rounds}_lr{lr}_sp{int(sparsity_ratio*100)}_calib{num_calib_rounds}\",\n",
        "    config={\n",
        "        \"model\": \"DINO ViT-S/16\",\n",
        "        \"K\": K,\n",
        "        \"C\": C,\n",
        "        \"J\": J,\n",
        "        \"n_rounds\": n_rounds,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr,\n",
        "        \"momentum\": momentum,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"sharding\": \"iid\",\n",
        "        \"Nc\": None,\n",
        "        \"use_sparse\": True,\n",
        "        \"sparsity_ratio\": sparsity_ratio,\n",
        "        \"num_calib_rounds\": num_calib_rounds,\n",
        "        \"sparse_ft_epochs\": sparse_ft_epochs\n",
        "    }\n",
        ")\n",
        "\n",
        "# --- Federated trainer FL-ready ---\n",
        "trainer = FederatedTrainer(\n",
        "    clients=clients,\n",
        "    global_model=global_model,\n",
        "    device=device,\n",
        "    client_fraction=C,\n",
        "    local_epochs=J,\n",
        "    batch_size=batch_size,\n",
        "    lr=lr,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    scheduler_fn=make_scheduler,\n",
        "    use_sparse=True,\n",
        "    sparsity_ratio=sparsity_ratio,\n",
        "    num_calib_rounds=num_calib_rounds,\n",
        "    sparse_ft_epochs=sparse_ft_epochs\n",
        ")\n",
        "\n",
        "# --- Patch and run ---\n",
        "trainer.fit = types.MethodType(fit_with_all_logs, trainer)\n",
        "trainer.fit(n_rounds, eval_fn=eval_fn_test, eval_every=2, checkpoint_path=CHECKPOINT_PATH, resume=True)\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "A5HxyyVI_Uab"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Test Accuracy FedAvg non IID Model Editing"
      ],
      "metadata": {
        "id": "MyEDSn34-ciW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final Evaluation of Model Editing under Non-IID Setup (Test Accuracy)\n",
        "\n",
        "This experiment evaluates the final performance of the **Model Editing strategy** in a **Non-IID federated learning scenario**, where each client receives a biased subset of CIFAR-100 (Nc = 50 classes per client).\n",
        "\n",
        "### Key Features:\n",
        "- **Non-IID Distribution**: Each client receives data from only `Nc` distinct classes, simulating strong statistical heterogeneity.\n",
        "- **Model Editing Technique**: Gradient masking is used to restrict training to only 10% of the model’s parameters (`sparsity_ratio = 0.90`), calibrated over `5` rounds. Each sparse fine-tuning phase runs for `1` local epoch.\n",
        "- **Global Evaluation**: The performance is assessed on the full CIFAR-100 test set, allowing comparison with other baselines.\n",
        "- **Backbone Model**: A DINO-pretrained ViT-S/16 with frozen feature extractor and learnable classifier head.\n",
        "- **Logging & Visualization**: Test accuracy and loss are reported every 2 rounds using Weights & Biases. Checkpoints and plots help monitor convergence and performance over time.\n",
        "\n"
      ],
      "metadata": {
        "id": "N73nr8vsvi8R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ====== FL READY: DATA, CLIENTS, WANDB, MODEL EDITING NON-IID TEST ACC ======\n",
        "\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "import numpy as np\n",
        "import wandb\n",
        "import types\n",
        "import os\n",
        "import json\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# --- FL params ---\n",
        "K = 100\n",
        "C = 0.1\n",
        "J = 4\n",
        "n_rounds = 50\n",
        "batch_size = 128\n",
        "lr = 0.001\n",
        "momentum = 0.8\n",
        "weight_decay = 5e-4\n",
        "Nc = Nc\n",
        "\n",
        "# --- Model Editing HP choosen ---\n",
        "sparsity_ratio = 0.90\n",
        "num_calib_rounds = 5\n",
        "sparse_ft_epochs = 1\n",
        "\n",
        "CHECKPOINT_PATH = \"model_editing_non_iid_checkpoint.pt\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# --- Transforms (ImageNet style for ViT/DINO) ---\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.RandomCrop(224, padding=4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "# --- Chargement CIFAR-100 brut (pas de transform ici) ---\n",
        "full_train = torchvision.datasets.CIFAR100(root=\"./data\", train=True, download=True)\n",
        "test_set = torchvision.datasets.CIFAR100(root=\"./data\", train=False, download=True)\n",
        "\n",
        "# --- FL split: NON-IID sharding + train/val local ---\n",
        "client_data = non_iid_split  # Utilise ton split NON-IID fixé\n",
        "\n",
        "from torch.utils.data import Subset, Dataset\n",
        "\n",
        "class TransformedSubset(Dataset):\n",
        "    def __init__(self, subset, transform):\n",
        "        self.subset = subset\n",
        "        self.transform = transform\n",
        "    def __getitem__(self, idx):\n",
        "        x, y = self.subset[idx]\n",
        "        x = self.transform(x)\n",
        "        return x, y\n",
        "    def __len__(self):\n",
        "        return len(self.subset)\n",
        "\n",
        "clients = []\n",
        "for i in range(K):\n",
        "    train_idxs = client_data[i]['train']\n",
        "    client_train_dataset = TransformedSubset(Subset(full_train, train_idxs), train_transform)\n",
        "    clients.append(Client(i, client_train_dataset, device))\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    TransformedSubset(test_set, val_transform), batch_size=128, shuffle=False\n",
        ")\n",
        "\n",
        "# --- Global model ViT-S/16 DINO CIFAR-100 ---\n",
        "global_model = DinoViT_CIFAR100(num_classes=100).to(device)\n",
        "\n",
        "def make_scheduler(optimizer):\n",
        "    return torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=J)\n",
        "\n",
        "def eval_fn_test(model):\n",
        "    return evaluate(model, test_loader, device)\n",
        "\n",
        "def save_checkpoint(model, round_idx, acc_history, loss_history, path=CHECKPOINT_PATH):\n",
        "    checkpoint = {\n",
        "        \"round\": round_idx,\n",
        "        \"model_state\": model.state_dict(),\n",
        "        \"acc_history\": acc_history,\n",
        "        \"loss_history\": loss_history\n",
        "    }\n",
        "    torch.save(checkpoint, path)\n",
        "\n",
        "def load_checkpoint(model, path=CHECKPOINT_PATH):\n",
        "    if os.path.exists(path):\n",
        "        checkpoint = torch.load(path, map_location=device)\n",
        "        model.load_state_dict(checkpoint[\"model_state\"])\n",
        "        print(f\"Checkpoint loaded (round {checkpoint['round']})\")\n",
        "        return checkpoint[\"round\"], checkpoint[\"acc_history\"], checkpoint[\"loss_history\"]\n",
        "    return 0, [], []\n",
        "\n",
        "def plot_history(acc_history, loss_history, eval_every):\n",
        "    rounds = np.arange(0, len(acc_history))*eval_every + eval_every\n",
        "    plt.figure(figsize=(8,4))\n",
        "    plt.subplot(1,2,1)\n",
        "    plt.plot(rounds, acc_history, label='Test Acc')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.title('Test Accuracy')\n",
        "    plt.subplot(1,2,2)\n",
        "    plt.plot(rounds, loss_history, label='Test Loss')\n",
        "    plt.xlabel('Round')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Test Loss')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def fit_with_all_logs(self, n_rounds, eval_fn=None, eval_every=1, checkpoint_path=CHECKPOINT_PATH, resume=False):\n",
        "    start_round, acc_history, loss_history = (0, [], [])\n",
        "    if resume:\n",
        "        start_round, acc_history, loss_history = load_checkpoint(self.global_model, checkpoint_path)\n",
        "    for rnd in range(start_round+1, n_rounds+1):\n",
        "        print(f'---- FedAvg Round {rnd} (MODEL EDITING NON-IID) ----')\n",
        "        self.train_round()\n",
        "        if eval_fn and rnd % eval_every == 0:\n",
        "            test_acc, test_loss = eval_fn(self.global_model)\n",
        "            print(f'[Round {rnd}] Test Acc={test_acc:.3f} | Test Loss={test_loss:.3f}')\n",
        "            wandb.log({\"round\": rnd, \"test_acc\": test_acc, \"test_loss\": test_loss})\n",
        "            acc_history.append(test_acc)\n",
        "            loss_history.append(test_loss)\n",
        "            if rnd % 5 == 0 or rnd == n_rounds:\n",
        "                save_checkpoint(self.global_model, rnd, acc_history, loss_history, checkpoint_path)\n",
        "        if rnd % 5 == 0 or rnd == n_rounds:\n",
        "            plot_history(acc_history, loss_history, eval_every)\n",
        "\n",
        "# --- WANDB init ---\n",
        "wandb.init(\n",
        "    project=\"fl-fedavg\",\n",
        "    name=f\"model_editing_non_iid_test_acc_J{J}_nrounds{n_rounds}_lr{lr}_sp{int(sparsity_ratio*100)}_calib{num_calib_rounds}\",\n",
        "    config={\n",
        "        \"model\": \"DINO ViT-S/16\",\n",
        "        \"K\": K,\n",
        "        \"C\": C,\n",
        "        \"J\": J,\n",
        "        \"n_rounds\": n_rounds,\n",
        "        \"batch_size\": batch_size,\n",
        "        \"lr\": lr,\n",
        "        \"momentum\": momentum,\n",
        "        \"weight_decay\": weight_decay,\n",
        "        \"sharding\": \"non-iid\",\n",
        "        \"Nc\": Nc,\n",
        "        \"use_sparse\": True,\n",
        "        \"sparsity_ratio\": sparsity_ratio,\n",
        "        \"num_calib_rounds\": num_calib_rounds,\n",
        "        \"sparse_ft_epochs\": sparse_ft_epochs\n",
        "    }\n",
        ")\n",
        "\n",
        "# --- Federated trainer FL-ready ---\n",
        "trainer = FederatedTrainer(\n",
        "    clients=clients,\n",
        "    global_model=global_model,\n",
        "    device=device,\n",
        "    client_fraction=C,\n",
        "    local_epochs=J,\n",
        "    batch_size=batch_size,\n",
        "    lr=lr,\n",
        "    momentum=momentum,\n",
        "    weight_decay=weight_decay,\n",
        "    scheduler_fn=make_scheduler,\n",
        "    use_sparse=True,\n",
        "    sparsity_ratio=sparsity_ratio,\n",
        "    num_calib_rounds=num_calib_rounds,\n",
        "    sparse_ft_epochs=sparse_ft_epochs\n",
        ")\n",
        "\n",
        "# --- Patch and run ---\n",
        "trainer.fit = types.MethodType(fit_with_all_logs, trainer)\n",
        "trainer.fit(n_rounds, eval_fn=eval_fn_test, eval_every=2, checkpoint_path=CHECKPOINT_PATH, resume=True)\n",
        "wandb.finish()\n"
      ],
      "metadata": {
        "id": "SCxi4fXC-zFw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}